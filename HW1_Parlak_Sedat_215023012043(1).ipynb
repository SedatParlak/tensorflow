{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O_-dYA9ZJRe"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1hwCZMi7Fxv"
      },
      "source": [
        "Experiments with Deep Neural Networks. \n",
        "You are expected to use the Python language and Keras library unless otherwise noted. You will \n",
        "prepare a report including your code and results (in a Jupyter Notebook). The expected report format\n",
        "is shown at the end of this document. You are encouraged to use Google Colab for computational\n",
        "needs to do this work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USPKI51J9B30"
      },
      "source": [
        "**Part 1: Model a deep feed forward network for regression**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSo2AK4A9LlB"
      },
      "source": [
        "Assume that you are given a polynomial with multiple inputs and multiple outputs of the form: \n",
        "\n",
        "(ùë¶1, ùë¶2, ‚Ä¶ , ùë¶6) = ùëÉ(ùë•1, ùë•2, ‚Ä¶ , ùë•8)\n",
        "\n",
        "The exact polynomials are: \n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-d31YWO-MsB"
      },
      "source": [
        "For this given set of polynomials, generate ùëÅ\n",
        "ùë°\n",
        " instances to train a network. Create an additional ùëÅ\n",
        "ùë£\n",
        "\n",
        "instances for validation of the trained model.  \n",
        "1. Choose ùëÅ\n",
        "ùë°\n",
        " to be 1000.\n",
        "2. In your training data add some noise to ùë¶\n",
        "ùëñ\n",
        "‚Äôs from a normal distribution with ùúá = 0.0 and ùúé =\n",
        "0.001. \n",
        "3. Build a feed forward network with exactly 3 hidden layers:\n",
        "* Each layer should include exactly 6 nodes in the beginning.\n",
        "* Use a combination of activation functions in these layers (use the same activation for \n",
        "each node at a given layer).\n",
        "4. Define your loss function: \n",
        "* Use MSE for loss function.\n",
        "5. Train your algorithm with SGD. \n",
        "* Use appropriate learning rates and the number of epochs.\n",
        "* Report the training and validation errors. \n",
        "6. Repeat Steps 2-4 with another set of activation functions (3 different combinations), learning\n",
        "rates (3 different schemes) and number of epochs (after finding a reasonable number of\n",
        "epochs in the first trial, increase by 50% for 2 times). \n",
        "7. Choose your best parameters after Step 5.\n",
        "8. Add new nodes at a time to each hidden layer: \n",
        "* Start from the first hidden layer, add two nodes, train, and record results.\n",
        "* Move to the second hidden layer, add two nodes, train, and record results.\n",
        "* Move to the third hidden layer, add two nodes, train, and record results.\n",
        "* Repeat Step 8 until bias and variance curve is drawn (see Figure 1 for a fictitious \n",
        "example from the first lecture).\n",
        "9. Increase ùëÅ\n",
        "ùë°\n",
        " by 10% and repeat Step 8.\n",
        "10. Report your all results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTR4BHrk_Yp8"
      },
      "source": [
        "# Step 1 to Step 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvf5LDlF_gPe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptzdYURL_u4o"
      },
      "outputs": [],
      "source": [
        "# Polynƒ±mial\n",
        "def polynomial(x):\n",
        "    y1 = (x[0]*x[1]*x[2]) + (1.2*x[3]*x[4]) - (x[5]*x[6]*x[7]) - (3*(x[0]**2)*x[7]) + (2*x[4])\n",
        "    y2 = (x[0]*x[4]*x[5]) - (x[2]*x[3]) - (3*x[1]*x[2]) - (0.2*(x[1]**2)*x[3]) - (2*x[6]*x[7]) + 1\n",
        "    y3 = (x[2]**2) - (x[4]*x[6]) - (3*x[0]*x[3]*x[5]) - ((x[0]**2)*x[1]*x[3]) - 2\n",
        "    y4 = (x[5]**3) - (2*x[0]*x[2]*x[7]) - (x[0]*x[3]*x[6]) - (2*(x[4]**2)*x[1]*x[3]) - x[7]\n",
        "    y5 = ((x[2]**2)*x[4]) - (3*x[2]*x[3]*x[7]) - (x[0]*x[1]*x[3]) - (3*x[5]) + ((x[0]**2)*x[6]) - 1\n",
        "    y6 = ((x[0]**2)*x[2]*x[5]) - (0.5*x[2]*x[4]*x[6]) + (x[0]*x[2]*x[3]) + (2.2*x[3]) + ((x[1]**2)*x[2]) + 1\n",
        "    return np.array([y1, y2, y3, y4, y5, y6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhnReanN_1sF"
      },
      "outputs": [],
      "source": [
        "# generate train data\n",
        "np.random.seed(0)\n",
        "Nt = 1000\n",
        "X_train = np.random.rand(Nt, 8)\n",
        "y_train = np.apply_along_axis(polynomial, axis=1, arr=X_train) + np.random.normal(0, 0.001, (Nt, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHymdl4goAon"
      },
      "outputs": [],
      "source": [
        "# generate validation data\n",
        "Nv = 350\n",
        "X_val = np.random.rand(Nv, 8)\n",
        "y_val = np.apply_along_axis(polynomial, axis=1, arr=X_val) + np.random.normal(0, 0.001, (Nv, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI3bhLDAvFrE",
        "outputId": "813a4b34-0e8a-4817-bbbb-34d48ad43967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 122/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1172 - val_loss: 0.1272\n",
            "Epoch 123/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1170 - val_loss: 0.1247\n",
            "Epoch 124/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1132 - val_loss: 0.1475\n",
            "Epoch 125/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1149 - val_loss: 0.1261\n",
            "Epoch 126/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 0.1236\n",
            "Epoch 127/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1142 - val_loss: 0.1251\n",
            "Epoch 128/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 0.1322\n",
            "Epoch 129/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1138 - val_loss: 0.1232\n",
            "Epoch 130/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1123 - val_loss: 0.1270\n",
            "Epoch 131/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1136 - val_loss: 0.1221\n",
            "Epoch 132/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1124 - val_loss: 0.1229\n",
            "Epoch 133/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1145 - val_loss: 0.1379\n",
            "Epoch 134/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1125 - val_loss: 0.1308\n",
            "Epoch 135/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 0.1425\n",
            "Epoch 136/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1135 - val_loss: 0.1330\n",
            "Epoch 137/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 0.1400\n",
            "Epoch 138/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1122 - val_loss: 0.1214\n",
            "Epoch 139/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1133 - val_loss: 0.1561\n",
            "Epoch 140/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1150 - val_loss: 0.1287\n",
            "Epoch 141/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1127 - val_loss: 0.1264\n",
            "Epoch 142/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1109 - val_loss: 0.1550\n",
            "Epoch 143/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1132 - val_loss: 0.1211\n",
            "Epoch 144/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1095 - val_loss: 0.1207\n",
            "Epoch 145/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1092 - val_loss: 0.1246\n",
            "Epoch 146/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1103 - val_loss: 0.1236\n",
            "Epoch 147/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1105 - val_loss: 0.1628\n",
            "Epoch 148/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 0.1199\n",
            "Epoch 149/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1107 - val_loss: 0.1298\n",
            "Epoch 150/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1110 - val_loss: 0.1211\n",
            "Epoch 151/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1086 - val_loss: 0.1307\n",
            "Epoch 152/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1114 - val_loss: 0.1209\n",
            "Epoch 153/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1102 - val_loss: 0.1358\n",
            "Epoch 154/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 0.1236\n",
            "Epoch 155/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1085 - val_loss: 0.1418\n",
            "Epoch 156/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 0.1362\n",
            "Epoch 157/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1090 - val_loss: 0.1290\n",
            "Epoch 158/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1080 - val_loss: 0.1175\n",
            "Epoch 159/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1088 - val_loss: 0.1191\n",
            "Epoch 160/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1076 - val_loss: 0.1464\n",
            "Epoch 161/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1100 - val_loss: 0.1209\n",
            "Epoch 162/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1071 - val_loss: 0.1276\n",
            "Epoch 163/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1056 - val_loss: 0.1178\n",
            "Epoch 164/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1040 - val_loss: 0.1259\n",
            "Epoch 165/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1043 - val_loss: 0.1197\n",
            "Epoch 166/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1047 - val_loss: 0.1180\n",
            "Epoch 167/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1063 - val_loss: 0.1220\n",
            "Epoch 168/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1034 - val_loss: 0.1141\n",
            "Epoch 169/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1029 - val_loss: 0.1625\n",
            "Epoch 170/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1056 - val_loss: 0.1441\n",
            "Epoch 171/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1045 - val_loss: 0.1158\n",
            "Epoch 172/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1023 - val_loss: 0.1126\n",
            "Epoch 173/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1025 - val_loss: 0.1086\n",
            "Epoch 174/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0984 - val_loss: 0.1081\n",
            "Epoch 175/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0979 - val_loss: 0.1145\n",
            "Epoch 176/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 0.1109\n",
            "Epoch 177/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0960 - val_loss: 0.1397\n",
            "Epoch 178/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0969 - val_loss: 0.1025\n",
            "Epoch 179/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0945 - val_loss: 0.1069\n",
            "Epoch 180/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0945 - val_loss: 0.1037\n",
            "Epoch 181/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0933 - val_loss: 0.1120\n",
            "Epoch 182/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0935 - val_loss: 0.1058\n",
            "Epoch 183/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0911 - val_loss: 0.0980\n",
            "Epoch 184/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0894 - val_loss: 0.1080\n",
            "Epoch 185/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0887 - val_loss: 0.1059\n",
            "Epoch 186/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0884 - val_loss: 0.1096\n",
            "Epoch 187/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0882 - val_loss: 0.0971\n",
            "Epoch 188/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0871 - val_loss: 0.1142\n",
            "Epoch 189/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0882 - val_loss: 0.0929\n",
            "Epoch 190/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0844 - val_loss: 0.0934\n",
            "Epoch 191/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0848 - val_loss: 0.0969\n",
            "Epoch 192/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0839 - val_loss: 0.0918\n",
            "Epoch 193/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0828 - val_loss: 0.0951\n",
            "Epoch 194/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 0.0897\n",
            "Epoch 195/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0817 - val_loss: 0.1059\n",
            "Epoch 196/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0890\n",
            "Epoch 197/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.0886\n",
            "Epoch 198/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0826 - val_loss: 0.1080\n",
            "Epoch 199/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 0.0889\n",
            "Epoch 200/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0827 - val_loss: 0.0994\n",
            "Epoch 201/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 0.0868\n",
            "Epoch 202/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 0.1076\n",
            "Epoch 203/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0799 - val_loss: 0.0966\n",
            "Epoch 204/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0797 - val_loss: 0.0967\n",
            "Epoch 205/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0806 - val_loss: 0.0899\n",
            "Epoch 206/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 0.0862\n",
            "Epoch 207/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.0955\n",
            "Epoch 208/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0798 - val_loss: 0.0903\n",
            "Epoch 209/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0806 - val_loss: 0.1008\n",
            "Epoch 210/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0818 - val_loss: 0.0893\n",
            "Epoch 211/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0944\n",
            "Epoch 212/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 0.1095\n",
            "Epoch 213/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0800 - val_loss: 0.1054\n",
            "Epoch 214/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 0.0967\n",
            "Epoch 215/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0852\n",
            "Epoch 216/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0865\n",
            "Epoch 217/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0794 - val_loss: 0.0939\n",
            "Epoch 218/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0805 - val_loss: 0.0831\n",
            "Epoch 219/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0766 - val_loss: 0.0902\n",
            "Epoch 220/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.0821\n",
            "Epoch 221/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0765 - val_loss: 0.0950\n",
            "Epoch 222/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 0.0926\n",
            "Epoch 223/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.0971\n",
            "Epoch 224/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0782 - val_loss: 0.0872\n",
            "Epoch 225/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.0924\n",
            "Model with tanh activation, 100 epoch and 0.1 learning rate\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 1.0427 - val_loss: 0.6903\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6280 - val_loss: 0.6485\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5916 - val_loss: 0.6109\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5563 - val_loss: 0.5792\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5163 - val_loss: 0.5251\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4603 - val_loss: 0.4545\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4041 - val_loss: 0.4087\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3778 - val_loss: 0.3790\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3433 - val_loss: 0.3520\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3243 - val_loss: 0.3600\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2818 - val_loss: 0.2882\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2635 - val_loss: 0.3173\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2354 - val_loss: 0.2253\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1971 - val_loss: 0.1986\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1823 - val_loss: 0.1922\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1717 - val_loss: 0.2039\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1693 - val_loss: 0.1689\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1493 - val_loss: 0.2013\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1560 - val_loss: 0.1634\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1508 - val_loss: 0.1574\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1399 - val_loss: 0.1550\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1306 - val_loss: 0.1436\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1307 - val_loss: 0.1450\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1314 - val_loss: 0.1524\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1293 - val_loss: 0.1281\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1174 - val_loss: 0.1251\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1097 - val_loss: 0.1231\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1058 - val_loss: 0.1397\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1057 - val_loss: 0.1171\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.1278\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 0.1134\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0961 - val_loss: 0.1106\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1054 - val_loss: 0.1281\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.1096\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0964 - val_loss: 0.2097\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 0.1188\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0964 - val_loss: 0.1242\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0936 - val_loss: 0.1234\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0963 - val_loss: 0.1110\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 0.1285\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0909 - val_loss: 0.0983\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.0998\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0865 - val_loss: 0.0959\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.1005\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0874 - val_loss: 0.1034\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0850 - val_loss: 0.1027\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0881 - val_loss: 0.1041\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0859 - val_loss: 0.0975\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0846 - val_loss: 0.0942\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0826 - val_loss: 0.0989\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0888 - val_loss: 0.1149\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0906 - val_loss: 0.1028\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0872 - val_loss: 0.1037\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0839 - val_loss: 0.1157\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0870 - val_loss: 0.0950\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0805 - val_loss: 0.0949\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0840 - val_loss: 0.0979\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0789 - val_loss: 0.1066\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.0941\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0837 - val_loss: 0.0860\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0787 - val_loss: 0.1176\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0795 - val_loss: 0.0848\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0768 - val_loss: 0.0998\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0782 - val_loss: 0.1230\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0808 - val_loss: 0.0931\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0803 - val_loss: 0.0827\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0862\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0881\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0778 - val_loss: 0.1010\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0768 - val_loss: 0.0852\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.0872\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0744 - val_loss: 0.0867\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0834\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0745 - val_loss: 0.0814\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0742 - val_loss: 0.0813\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0753 - val_loss: 0.1126\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 0.0997\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0825\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 0.0943\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 0.0780\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0784\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0885\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0807\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0890\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0785\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0703 - val_loss: 0.0867\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0981\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0770\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.0829\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.0839\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.0900\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0680 - val_loss: 0.0952\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0692 - val_loss: 0.0956\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.0777\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0698 - val_loss: 0.0795\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0694 - val_loss: 0.0718\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0737\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0683 - val_loss: 0.0726\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0709 - val_loss: 0.0942\n",
            "Model with tanh activation, 150 epoch and 0.1 learning rate\n",
            "Epoch 1/150\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 1.3088 - val_loss: 0.6997\n",
            "Epoch 2/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6406 - val_loss: 0.6416\n",
            "Epoch 3/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5690 - val_loss: 0.5653\n",
            "Epoch 4/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5035 - val_loss: 0.5217\n",
            "Epoch 5/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4565 - val_loss: 0.4683\n",
            "Epoch 6/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4157 - val_loss: 0.4345\n",
            "Epoch 7/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3793 - val_loss: 0.3889\n",
            "Epoch 8/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3473 - val_loss: 0.4005\n",
            "Epoch 9/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3180 - val_loss: 0.3286\n",
            "Epoch 10/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2934 - val_loss: 0.3026\n",
            "Epoch 11/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2715 - val_loss: 0.2883\n",
            "Epoch 12/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2425 - val_loss: 0.2489\n",
            "Epoch 13/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2091 - val_loss: 0.2232\n",
            "Epoch 14/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1895 - val_loss: 0.2102\n",
            "Epoch 15/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1835 - val_loss: 0.1924\n",
            "Epoch 16/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1631 - val_loss: 0.2439\n",
            "Epoch 17/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1663 - val_loss: 0.1761\n",
            "Epoch 18/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1672 - val_loss: 0.1766\n",
            "Epoch 19/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1452 - val_loss: 0.1875\n",
            "Epoch 20/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1537 - val_loss: 0.1981\n",
            "Epoch 21/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1522 - val_loss: 0.1621\n",
            "Epoch 22/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1465 - val_loss: 0.1584\n",
            "Epoch 23/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1498 - val_loss: 0.1649\n",
            "Epoch 24/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1368 - val_loss: 0.1561\n",
            "Epoch 25/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1379 - val_loss: 0.1696\n",
            "Epoch 26/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1511 - val_loss: 0.1591\n",
            "Epoch 27/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1367 - val_loss: 0.1842\n",
            "Epoch 28/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1428 - val_loss: 0.1522\n",
            "Epoch 29/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1302 - val_loss: 0.1433\n",
            "Epoch 30/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1306 - val_loss: 0.2146\n",
            "Epoch 31/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1347 - val_loss: 0.1616\n",
            "Epoch 32/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1324 - val_loss: 0.1522\n",
            "Epoch 33/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1278 - val_loss: 0.1445\n",
            "Epoch 34/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 0.1487\n",
            "Epoch 35/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1216 - val_loss: 0.1586\n",
            "Epoch 36/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1217 - val_loss: 0.1721\n",
            "Epoch 37/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1181 - val_loss: 0.1333\n",
            "Epoch 38/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1146 - val_loss: 0.2299\n",
            "Epoch 39/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 0.1380\n",
            "Epoch 40/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1156 - val_loss: 0.1400\n",
            "Epoch 41/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 0.1383\n",
            "Epoch 42/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1139 - val_loss: 0.1218\n",
            "Epoch 43/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1090 - val_loss: 0.1307\n",
            "Epoch 44/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1095 - val_loss: 0.1594\n",
            "Epoch 45/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1118 - val_loss: 0.1204\n",
            "Epoch 46/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1113 - val_loss: 0.1273\n",
            "Epoch 47/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1091 - val_loss: 0.1265\n",
            "Epoch 48/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1083 - val_loss: 0.1177\n",
            "Epoch 49/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1120 - val_loss: 0.1538\n",
            "Epoch 50/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1105 - val_loss: 0.1442\n",
            "Epoch 51/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1051 - val_loss: 0.1299\n",
            "Epoch 52/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1049 - val_loss: 0.1138\n",
            "Epoch 53/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1074 - val_loss: 0.1198\n",
            "Epoch 54/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1026 - val_loss: 0.1096\n",
            "Epoch 55/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0996 - val_loss: 0.1493\n",
            "Epoch 56/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1031 - val_loss: 0.1276\n",
            "Epoch 57/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1002 - val_loss: 0.1110\n",
            "Epoch 58/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0998 - val_loss: 0.1065\n",
            "Epoch 59/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1028 - val_loss: 0.1147\n",
            "Epoch 60/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.1215\n",
            "Epoch 61/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1007 - val_loss: 0.1107\n",
            "Epoch 62/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0956 - val_loss: 0.1088\n",
            "Epoch 63/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0960 - val_loss: 0.1120\n",
            "Epoch 64/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0956 - val_loss: 0.1219\n",
            "Epoch 65/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0940 - val_loss: 0.1049\n",
            "Epoch 66/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0949 - val_loss: 0.1127\n",
            "Epoch 67/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0985 - val_loss: 0.1024\n",
            "Epoch 68/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0920 - val_loss: 0.1672\n",
            "Epoch 69/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0957 - val_loss: 0.1284\n",
            "Epoch 70/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0950 - val_loss: 0.0995\n",
            "Epoch 71/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0898 - val_loss: 0.1141\n",
            "Epoch 72/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0938 - val_loss: 0.1185\n",
            "Epoch 73/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0947 - val_loss: 0.1362\n",
            "Epoch 74/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0909 - val_loss: 0.0985\n",
            "Epoch 75/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0886 - val_loss: 0.1236\n",
            "Epoch 76/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0912 - val_loss: 0.0995\n",
            "Epoch 77/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0890 - val_loss: 0.0939\n",
            "Epoch 78/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.1064\n",
            "Epoch 79/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0861 - val_loss: 0.0954\n",
            "Epoch 80/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0880 - val_loss: 0.0917\n",
            "Epoch 81/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0870 - val_loss: 0.1011\n",
            "Epoch 82/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.0978\n",
            "Epoch 83/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 0.1274\n",
            "Epoch 84/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.0880\n",
            "Epoch 85/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.0858\n",
            "Epoch 86/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0834 - val_loss: 0.0934\n",
            "Epoch 87/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0823 - val_loss: 0.0891\n",
            "Epoch 88/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0819 - val_loss: 0.0833\n",
            "Epoch 89/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0904\n",
            "Epoch 90/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0815 - val_loss: 0.1140\n",
            "Epoch 91/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 0.0943\n",
            "Epoch 92/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0857 - val_loss: 0.1144\n",
            "Epoch 93/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0816 - val_loss: 0.0880\n",
            "Epoch 94/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0833 - val_loss: 0.0959\n",
            "Epoch 95/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0823 - val_loss: 0.0855\n",
            "Epoch 96/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0799 - val_loss: 0.0798\n",
            "Epoch 97/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.0823\n",
            "Epoch 98/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0806 - val_loss: 0.0908\n",
            "Epoch 99/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0769 - val_loss: 0.1021\n",
            "Epoch 100/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0914\n",
            "Epoch 101/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0823\n",
            "Epoch 102/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0825 - val_loss: 0.0880\n",
            "Epoch 103/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0806 - val_loss: 0.0932\n",
            "Epoch 104/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.1167\n",
            "Epoch 105/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0832 - val_loss: 0.0817\n",
            "Epoch 106/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0920\n",
            "Epoch 107/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0922\n",
            "Epoch 108/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0769 - val_loss: 0.0890\n",
            "Epoch 109/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0947\n",
            "Epoch 110/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0780\n",
            "Epoch 111/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0756 - val_loss: 0.0799\n",
            "Epoch 112/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0747 - val_loss: 0.0864\n",
            "Epoch 113/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0813\n",
            "Epoch 114/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0746\n",
            "Epoch 115/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0743 - val_loss: 0.0801\n",
            "Epoch 116/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0748 - val_loss: 0.1035\n",
            "Epoch 117/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0801 - val_loss: 0.0822\n",
            "Epoch 118/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0751 - val_loss: 0.0754\n",
            "Epoch 119/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0720 - val_loss: 0.1011\n",
            "Epoch 120/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 0.0920\n",
            "Epoch 121/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0756 - val_loss: 0.0751\n",
            "Epoch 122/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0745 - val_loss: 0.0848\n",
            "Epoch 123/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0797 - val_loss: 0.0732\n",
            "Epoch 124/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0990\n",
            "Epoch 125/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.0873\n",
            "Epoch 126/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0792 - val_loss: 0.0773\n",
            "Epoch 127/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0769\n",
            "Epoch 128/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 0.0798\n",
            "Epoch 129/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0750 - val_loss: 0.0832\n",
            "Epoch 130/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0758\n",
            "Epoch 131/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0847\n",
            "Epoch 132/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0757 - val_loss: 0.0923\n",
            "Epoch 133/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0739\n",
            "Epoch 134/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0794\n",
            "Epoch 135/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0807\n",
            "Epoch 136/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 0.0752\n",
            "Epoch 137/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0777\n",
            "Epoch 138/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0751\n",
            "Epoch 139/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0704 - val_loss: 0.0752\n",
            "Epoch 140/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0787 - val_loss: 0.0919\n",
            "Epoch 141/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0739 - val_loss: 0.0847\n",
            "Epoch 142/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0701 - val_loss: 0.0766\n",
            "Epoch 143/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 0.0854\n",
            "Epoch 144/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0701 - val_loss: 0.0809\n",
            "Epoch 145/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0802\n",
            "Epoch 146/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 0.0755\n",
            "Epoch 147/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0687 - val_loss: 0.0731\n",
            "Epoch 148/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0764\n",
            "Epoch 149/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.0718\n",
            "Epoch 150/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.1076\n",
            "Model with tanh activation, 225 epoch and 0.1 learning rate\n",
            "Epoch 1/225\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 1.1045 - val_loss: 0.7171\n",
            "Epoch 2/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6638 - val_loss: 0.6927\n",
            "Epoch 3/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6335 - val_loss: 0.6524\n",
            "Epoch 4/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5892 - val_loss: 0.6028\n",
            "Epoch 5/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5307 - val_loss: 0.5458\n",
            "Epoch 6/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4673 - val_loss: 0.4635\n",
            "Epoch 7/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4137 - val_loss: 0.4319\n",
            "Epoch 8/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3807 - val_loss: 0.3880\n",
            "Epoch 9/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3640 - val_loss: 0.4067\n",
            "Epoch 10/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3418 - val_loss: 0.3431\n",
            "Epoch 11/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3168 - val_loss: 0.3184\n",
            "Epoch 12/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2826 - val_loss: 0.2998\n",
            "Epoch 13/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2686 - val_loss: 0.3543\n",
            "Epoch 14/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2427 - val_loss: 0.2591\n",
            "Epoch 15/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2366 - val_loss: 0.2796\n",
            "Epoch 16/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2285 - val_loss: 0.2414\n",
            "Epoch 17/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2322 - val_loss: 0.2292\n",
            "Epoch 18/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1941 - val_loss: 0.2155\n",
            "Epoch 19/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2082 - val_loss: 0.2278\n",
            "Epoch 20/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1823 - val_loss: 0.2026\n",
            "Epoch 21/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1892 - val_loss: 0.2273\n",
            "Epoch 22/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1777 - val_loss: 0.2043\n",
            "Epoch 23/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1845 - val_loss: 0.1836\n",
            "Epoch 24/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1684 - val_loss: 0.1793\n",
            "Epoch 25/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1677 - val_loss: 0.1756\n",
            "Epoch 26/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1559 - val_loss: 0.1739\n",
            "Epoch 27/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1573 - val_loss: 0.2176\n",
            "Epoch 28/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1623 - val_loss: 0.1704\n",
            "Epoch 29/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1389 - val_loss: 0.1575\n",
            "Epoch 30/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1325 - val_loss: 0.1464\n",
            "Epoch 31/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1414 - val_loss: 0.1602\n",
            "Epoch 32/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1328 - val_loss: 0.1470\n",
            "Epoch 33/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1308 - val_loss: 0.1597\n",
            "Epoch 34/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1219 - val_loss: 0.1564\n",
            "Epoch 35/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1554\n",
            "Epoch 36/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1132 - val_loss: 0.1425\n",
            "Epoch 37/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1114 - val_loss: 0.1461\n",
            "Epoch 38/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1218 - val_loss: 0.1303\n",
            "Epoch 39/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 0.1420\n",
            "Epoch 40/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 0.1937\n",
            "Epoch 41/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1140 - val_loss: 0.1259\n",
            "Epoch 42/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1106 - val_loss: 0.1329\n",
            "Epoch 43/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 0.1209\n",
            "Epoch 44/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1016 - val_loss: 0.1156\n",
            "Epoch 45/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1033 - val_loss: 0.1155\n",
            "Epoch 46/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1035 - val_loss: 0.1255\n",
            "Epoch 47/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0986 - val_loss: 0.1143\n",
            "Epoch 48/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1072 - val_loss: 0.1185\n",
            "Epoch 49/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0960 - val_loss: 0.1117\n",
            "Epoch 50/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0938 - val_loss: 0.1137\n",
            "Epoch 51/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0937 - val_loss: 0.1162\n",
            "Epoch 52/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0940 - val_loss: 0.1190\n",
            "Epoch 53/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1010 - val_loss: 0.1148\n",
            "Epoch 54/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0939 - val_loss: 0.1067\n",
            "Epoch 55/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0916 - val_loss: 0.1044\n",
            "Epoch 56/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1001 - val_loss: 0.1096\n",
            "Epoch 57/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0877 - val_loss: 0.1134\n",
            "Epoch 58/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0950 - val_loss: 0.1010\n",
            "Epoch 59/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0904 - val_loss: 0.1043\n",
            "Epoch 60/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0922 - val_loss: 0.1114\n",
            "Epoch 61/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0865 - val_loss: 0.0968\n",
            "Epoch 62/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0883 - val_loss: 0.0983\n",
            "Epoch 63/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 0.1011\n",
            "Epoch 64/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0849 - val_loss: 0.1038\n",
            "Epoch 65/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0855 - val_loss: 0.1179\n",
            "Epoch 66/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0868 - val_loss: 0.0956\n",
            "Epoch 67/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0839 - val_loss: 0.1305\n",
            "Epoch 68/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0955 - val_loss: 0.0943\n",
            "Epoch 69/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0899 - val_loss: 0.1317\n",
            "Epoch 70/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 0.0997\n",
            "Epoch 71/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0838 - val_loss: 0.1200\n",
            "Epoch 72/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.1069\n",
            "Epoch 73/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0828 - val_loss: 0.1110\n",
            "Epoch 74/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0820 - val_loss: 0.0925\n",
            "Epoch 75/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0861 - val_loss: 0.0981\n",
            "Epoch 76/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.1125\n",
            "Epoch 77/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0841 - val_loss: 0.1255\n",
            "Epoch 78/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 0.0921\n",
            "Epoch 79/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0893\n",
            "Epoch 80/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0953\n",
            "Epoch 81/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0849 - val_loss: 0.0957\n",
            "Epoch 82/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0800 - val_loss: 0.1111\n",
            "Epoch 83/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.1111\n",
            "Epoch 84/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.0998\n",
            "Epoch 85/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0803 - val_loss: 0.0931\n",
            "Epoch 86/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0794 - val_loss: 0.1028\n",
            "Epoch 87/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0779 - val_loss: 0.1056\n",
            "Epoch 88/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 0.0918\n",
            "Epoch 89/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0865\n",
            "Epoch 90/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0908\n",
            "Epoch 91/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0766 - val_loss: 0.0926\n",
            "Epoch 92/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.1077\n",
            "Epoch 93/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0792 - val_loss: 0.0942\n",
            "Epoch 94/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0749 - val_loss: 0.0881\n",
            "Epoch 95/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0758 - val_loss: 0.0957\n",
            "Epoch 96/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0755 - val_loss: 0.0849\n",
            "Epoch 97/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0797 - val_loss: 0.0847\n",
            "Epoch 98/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0772 - val_loss: 0.1068\n",
            "Epoch 99/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.1093\n",
            "Epoch 100/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0765 - val_loss: 0.0820\n",
            "Epoch 101/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0895\n",
            "Epoch 102/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0931\n",
            "Epoch 103/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0785 - val_loss: 0.0900\n",
            "Epoch 104/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0816 - val_loss: 0.0868\n",
            "Epoch 105/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0953\n",
            "Epoch 106/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0746 - val_loss: 0.0865\n",
            "Epoch 107/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0765 - val_loss: 0.0818\n",
            "Epoch 108/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0729 - val_loss: 0.0997\n",
            "Epoch 109/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0800 - val_loss: 0.1118\n",
            "Epoch 110/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.0839\n",
            "Epoch 111/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0746 - val_loss: 0.0801\n",
            "Epoch 112/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0742 - val_loss: 0.0993\n",
            "Epoch 113/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0758 - val_loss: 0.0848\n",
            "Epoch 114/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0745 - val_loss: 0.0794\n",
            "Epoch 115/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0852\n",
            "Epoch 116/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0752 - val_loss: 0.0887\n",
            "Epoch 117/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0882\n",
            "Epoch 118/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0737 - val_loss: 0.0972\n",
            "Epoch 119/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.0808\n",
            "Epoch 120/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0994\n",
            "Epoch 121/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0743 - val_loss: 0.0933\n",
            "Epoch 122/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0942\n",
            "Epoch 123/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0760 - val_loss: 0.1023\n",
            "Epoch 124/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0922\n",
            "Epoch 125/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0759 - val_loss: 0.0817\n",
            "Epoch 126/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0807\n",
            "Epoch 127/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0792\n",
            "Epoch 128/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.0799\n",
            "Epoch 129/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.1116\n",
            "Epoch 130/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0862\n",
            "Epoch 131/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0704 - val_loss: 0.1045\n",
            "Epoch 132/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.0785\n",
            "Epoch 133/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0933\n",
            "Epoch 134/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0687 - val_loss: 0.1007\n",
            "Epoch 135/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0744 - val_loss: 0.0874\n",
            "Epoch 136/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0879\n",
            "Epoch 137/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.1091\n",
            "Epoch 138/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0963\n",
            "Epoch 139/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0964\n",
            "Epoch 140/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.0893\n",
            "Epoch 141/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0743 - val_loss: 0.0781\n",
            "Epoch 142/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0699 - val_loss: 0.1075\n",
            "Epoch 143/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0715 - val_loss: 0.1281\n",
            "Epoch 144/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 0.0939\n",
            "Epoch 145/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0714 - val_loss: 0.1145\n",
            "Epoch 146/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0751 - val_loss: 0.0894\n",
            "Epoch 147/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0817\n",
            "Epoch 148/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0747 - val_loss: 0.0752\n",
            "Epoch 149/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0976\n",
            "Epoch 150/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0875\n",
            "Epoch 151/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0709 - val_loss: 0.1107\n",
            "Epoch 152/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0940\n",
            "Epoch 153/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0934\n",
            "Epoch 154/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0891\n",
            "Epoch 155/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0707 - val_loss: 0.0801\n",
            "Epoch 156/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0685 - val_loss: 0.0858\n",
            "Epoch 157/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0834\n",
            "Epoch 158/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.0813\n",
            "Epoch 159/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0765\n",
            "Epoch 160/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0836\n",
            "Epoch 161/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0704 - val_loss: 0.0786\n",
            "Epoch 162/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0826\n",
            "Epoch 163/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0698 - val_loss: 0.0899\n",
            "Epoch 164/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0746\n",
            "Epoch 165/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0740 - val_loss: 0.0769\n",
            "Epoch 166/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0683 - val_loss: 0.0807\n",
            "Epoch 167/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0722 - val_loss: 0.0835\n",
            "Epoch 168/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0697 - val_loss: 0.0805\n",
            "Epoch 169/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0816\n",
            "Epoch 170/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.0854\n",
            "Epoch 171/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0723 - val_loss: 0.1039\n",
            "Epoch 172/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0704 - val_loss: 0.0875\n",
            "Epoch 173/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0701 - val_loss: 0.0744\n",
            "Epoch 174/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0656 - val_loss: 0.0760\n",
            "Epoch 175/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.0812\n",
            "Epoch 176/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0692 - val_loss: 0.0937\n",
            "Epoch 177/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0747 - val_loss: 0.0744\n",
            "Epoch 178/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0668 - val_loss: 0.0994\n",
            "Epoch 179/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0780\n",
            "Epoch 180/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.0740\n",
            "Epoch 181/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0817\n",
            "Epoch 182/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0677 - val_loss: 0.0832\n",
            "Epoch 183/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0694 - val_loss: 0.1159\n",
            "Epoch 184/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0777\n",
            "Epoch 185/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0676 - val_loss: 0.1219\n",
            "Epoch 186/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0686 - val_loss: 0.0770\n",
            "Epoch 187/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0691 - val_loss: 0.0831\n",
            "Epoch 188/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.0855\n",
            "Epoch 189/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.0754\n",
            "Epoch 190/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0680 - val_loss: 0.0710\n",
            "Epoch 191/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0955\n",
            "Epoch 192/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0758 - val_loss: 0.0732\n",
            "Epoch 193/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0697 - val_loss: 0.0782\n",
            "Epoch 194/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0663 - val_loss: 0.0918\n",
            "Epoch 195/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0671 - val_loss: 0.0793\n",
            "Epoch 196/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.0768\n",
            "Epoch 197/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0823\n",
            "Epoch 198/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0695 - val_loss: 0.0816\n",
            "Epoch 199/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.0791\n",
            "Epoch 200/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0701 - val_loss: 0.0736\n",
            "Epoch 201/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0669 - val_loss: 0.0975\n",
            "Epoch 202/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0683 - val_loss: 0.0761\n",
            "Epoch 203/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0679 - val_loss: 0.0725\n",
            "Epoch 204/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0666 - val_loss: 0.0782\n",
            "Epoch 205/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0755 - val_loss: 0.0736\n",
            "Epoch 206/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0668 - val_loss: 0.0718\n",
            "Epoch 207/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0786\n",
            "Epoch 208/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0664 - val_loss: 0.0774\n",
            "Epoch 209/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0676 - val_loss: 0.0800\n",
            "Epoch 210/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0676 - val_loss: 0.0711\n",
            "Epoch 211/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0663 - val_loss: 0.0840\n",
            "Epoch 212/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0647 - val_loss: 0.0776\n",
            "Epoch 213/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0672 - val_loss: 0.0828\n",
            "Epoch 214/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0670 - val_loss: 0.0772\n",
            "Epoch 215/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0770\n",
            "Epoch 216/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0782\n",
            "Epoch 217/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0688 - val_loss: 0.0920\n",
            "Epoch 218/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0758 - val_loss: 0.0765\n",
            "Epoch 219/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0643 - val_loss: 0.0774\n",
            "Epoch 220/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0649 - val_loss: 0.0758\n",
            "Epoch 221/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0681 - val_loss: 0.0875\n",
            "Epoch 222/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0664 - val_loss: 0.0827\n",
            "Epoch 223/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0670 - val_loss: 0.0751\n",
            "Epoch 224/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0704 - val_loss: 0.0927\n",
            "Epoch 225/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0896\n",
            "Model with tanh activation, 100 epoch and 0.2 learning rate\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.9440 - val_loss: 0.7271\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5774 - val_loss: 0.5924\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.5040\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4296 - val_loss: 0.4612\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4171 - val_loss: 0.4320\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4130 - val_loss: 0.4199\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3701 - val_loss: 0.4086\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3501 - val_loss: 0.4195\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2990 - val_loss: 0.3342\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2620 - val_loss: 0.2367\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1942 - val_loss: 0.2441\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1885 - val_loss: 0.2516\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1652 - val_loss: 0.2000\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1559 - val_loss: 0.1672\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1410 - val_loss: 0.1610\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 0.2411\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1391 - val_loss: 0.1503\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1515 - val_loss: 0.1407\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1270 - val_loss: 0.1720\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1233 - val_loss: 0.1314\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1150 - val_loss: 0.1229\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1113 - val_loss: 0.1442\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1183 - val_loss: 0.1229\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1135 - val_loss: 0.1442\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1018 - val_loss: 0.1075\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0933 - val_loss: 0.1087\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1049 - val_loss: 0.0994\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 0.1027\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.1318\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1141 - val_loss: 0.1645\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.1080\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.1072\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0944 - val_loss: 0.1187\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0829 - val_loss: 0.1247\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 0.1207\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0866 - val_loss: 0.1339\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0892 - val_loss: 0.1482\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0968 - val_loss: 0.0879\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.1449\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0859 - val_loss: 0.0911\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0975 - val_loss: 0.1123\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.0863\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0875 - val_loss: 0.0937\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0922 - val_loss: 0.1134\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.1302\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0878\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0805 - val_loss: 0.1019\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.1063\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0829 - val_loss: 0.0997\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0824 - val_loss: 0.1393\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0922 - val_loss: 0.0921\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0813 - val_loss: 0.1066\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 0.0942\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0868 - val_loss: 0.1040\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.0837\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 0.0847\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0744 - val_loss: 0.1246\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0795 - val_loss: 0.1171\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.1013\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.1010\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.1212\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 0.1021\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0779 - val_loss: 0.0927\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0844 - val_loss: 0.0928\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.1218\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0768 - val_loss: 0.1000\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 0.0817\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.0854\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0818 - val_loss: 0.1246\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0831 - val_loss: 0.0936\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0984\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0749 - val_loss: 0.0871\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0947 - val_loss: 0.1108\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0962\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0783 - val_loss: 0.1125\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 0.0791\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0776 - val_loss: 0.1731\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0749 - val_loss: 0.0959\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0755 - val_loss: 0.1245\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0882 - val_loss: 0.0992\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0751\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0835 - val_loss: 0.1000\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0822 - val_loss: 0.1354\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0806 - val_loss: 0.1201\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0832\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0916\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0750 - val_loss: 0.1033\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.0756\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.1499\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0997 - val_loss: 0.0993\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.1291\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0778 - val_loss: 0.1182\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0862 - val_loss: 0.1146\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0742 - val_loss: 0.0859\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0677 - val_loss: 0.1051\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0778 - val_loss: 0.0733\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0760 - val_loss: 0.0942\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0735 - val_loss: 0.1032\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0874 - val_loss: 0.0951\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0865\n",
            "Model with tanh activation, 150 epoch and 0.2 learning rate\n",
            "Epoch 1/150\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.9063 - val_loss: 0.6869\n",
            "Epoch 2/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6038 - val_loss: 0.7972\n",
            "Epoch 3/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5621 - val_loss: 0.6020\n",
            "Epoch 4/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5016 - val_loss: 0.4922\n",
            "Epoch 5/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4539 - val_loss: 0.5193\n",
            "Epoch 6/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3993 - val_loss: 0.4313\n",
            "Epoch 7/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3334 - val_loss: 0.3438\n",
            "Epoch 8/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3238 - val_loss: 0.3223\n",
            "Epoch 9/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 0.3664\n",
            "Epoch 10/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2859 - val_loss: 0.3088\n",
            "Epoch 11/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2575 - val_loss: 0.3033\n",
            "Epoch 12/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2620 - val_loss: 0.2665\n",
            "Epoch 13/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2429 - val_loss: 0.2813\n",
            "Epoch 14/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 0.2787\n",
            "Epoch 15/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2182 - val_loss: 0.2280\n",
            "Epoch 16/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2079 - val_loss: 0.2190\n",
            "Epoch 17/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1867 - val_loss: 0.2114\n",
            "Epoch 18/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1926 - val_loss: 0.1988\n",
            "Epoch 19/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1672 - val_loss: 0.3361\n",
            "Epoch 20/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1781 - val_loss: 0.2058\n",
            "Epoch 21/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1638 - val_loss: 0.2197\n",
            "Epoch 22/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1550 - val_loss: 0.2478\n",
            "Epoch 23/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1450 - val_loss: 0.1747\n",
            "Epoch 24/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1386 - val_loss: 0.1560\n",
            "Epoch 25/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1351 - val_loss: 0.1485\n",
            "Epoch 26/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 0.1939\n",
            "Epoch 27/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1380 - val_loss: 0.1549\n",
            "Epoch 28/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1291 - val_loss: 0.2481\n",
            "Epoch 29/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1384 - val_loss: 0.1700\n",
            "Epoch 30/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1264 - val_loss: 0.1834\n",
            "Epoch 31/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.1538\n",
            "Epoch 32/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1180 - val_loss: 0.1371\n",
            "Epoch 33/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1152 - val_loss: 0.2284\n",
            "Epoch 34/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1182 - val_loss: 0.1139\n",
            "Epoch 35/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 0.1458\n",
            "Epoch 36/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.1117\n",
            "Epoch 37/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.1625\n",
            "Epoch 38/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1030 - val_loss: 0.1505\n",
            "Epoch 39/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.1168\n",
            "Epoch 40/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1011 - val_loss: 0.1309\n",
            "Epoch 41/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 0.1063\n",
            "Epoch 42/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1060 - val_loss: 0.1111\n",
            "Epoch 43/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1079 - val_loss: 0.1644\n",
            "Epoch 44/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0942 - val_loss: 0.1539\n",
            "Epoch 45/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0964 - val_loss: 0.1074\n",
            "Epoch 46/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0938 - val_loss: 0.1156\n",
            "Epoch 47/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0929 - val_loss: 0.1534\n",
            "Epoch 48/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0953 - val_loss: 0.0979\n",
            "Epoch 49/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0899 - val_loss: 0.1003\n",
            "Epoch 50/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0875 - val_loss: 0.0953\n",
            "Epoch 51/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0913 - val_loss: 0.1040\n",
            "Epoch 52/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0892 - val_loss: 0.1176\n",
            "Epoch 53/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 0.1023\n",
            "Epoch 54/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 0.0978\n",
            "Epoch 55/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0872 - val_loss: 0.1117\n",
            "Epoch 56/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0916 - val_loss: 0.0961\n",
            "Epoch 57/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0981 - val_loss: 0.1015\n",
            "Epoch 58/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 0.1211\n",
            "Epoch 59/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - val_loss: 0.0960\n",
            "Epoch 60/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0889 - val_loss: 0.1360\n",
            "Epoch 61/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0963 - val_loss: 0.1066\n",
            "Epoch 62/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0859 - val_loss: 0.0982\n",
            "Epoch 63/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0881 - val_loss: 0.0876\n",
            "Epoch 64/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0798 - val_loss: 0.0972\n",
            "Epoch 65/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0853 - val_loss: 0.0937\n",
            "Epoch 66/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0865 - val_loss: 0.0847\n",
            "Epoch 67/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0862 - val_loss: 0.1630\n",
            "Epoch 68/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 0.1471\n",
            "Epoch 69/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0865 - val_loss: 0.0934\n",
            "Epoch 70/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0770 - val_loss: 0.0952\n",
            "Epoch 71/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0878 - val_loss: 0.0854\n",
            "Epoch 72/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.1113\n",
            "Epoch 73/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.1093\n",
            "Epoch 74/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 0.1103\n",
            "Epoch 75/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0821 - val_loss: 0.1070\n",
            "Epoch 76/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0840 - val_loss: 0.1160\n",
            "Epoch 77/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 0.1001\n",
            "Epoch 78/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0804 - val_loss: 0.1232\n",
            "Epoch 79/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0833 - val_loss: 0.0862\n",
            "Epoch 80/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0842\n",
            "Epoch 81/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0808 - val_loss: 0.0851\n",
            "Epoch 82/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0846 - val_loss: 0.0838\n",
            "Epoch 83/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0795 - val_loss: 0.0951\n",
            "Epoch 84/150\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0878 - val_loss: 0.0929\n",
            "Epoch 85/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0887\n",
            "Epoch 86/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0824 - val_loss: 0.0921\n",
            "Epoch 87/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0768 - val_loss: 0.1206\n",
            "Epoch 88/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0811 - val_loss: 0.1301\n",
            "Epoch 89/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0810 - val_loss: 0.0847\n",
            "Epoch 90/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0804 - val_loss: 0.1457\n",
            "Epoch 91/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0957 - val_loss: 0.0935\n",
            "Epoch 92/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.0928\n",
            "Epoch 93/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0765 - val_loss: 0.1129\n",
            "Epoch 94/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0807 - val_loss: 0.0914\n",
            "Epoch 95/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0777 - val_loss: 0.1017\n",
            "Epoch 96/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0796 - val_loss: 0.0854\n",
            "Epoch 97/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0824 - val_loss: 0.0834\n",
            "Epoch 98/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0778\n",
            "Epoch 99/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0764 - val_loss: 0.1136\n",
            "Epoch 100/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0795 - val_loss: 0.1120\n",
            "Epoch 101/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0809 - val_loss: 0.1141\n",
            "Epoch 102/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.0919\n",
            "Epoch 103/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.0935\n",
            "Epoch 104/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0829 - val_loss: 0.1046\n",
            "Epoch 105/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0818 - val_loss: 0.0840\n",
            "Epoch 106/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0744 - val_loss: 0.1076\n",
            "Epoch 107/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0804 - val_loss: 0.0745\n",
            "Epoch 108/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0778 - val_loss: 0.0823\n",
            "Epoch 109/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0789 - val_loss: 0.1277\n",
            "Epoch 110/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0861 - val_loss: 0.0905\n",
            "Epoch 111/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0840 - val_loss: 0.1038\n",
            "Epoch 112/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0823\n",
            "Epoch 113/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0747 - val_loss: 0.0809\n",
            "Epoch 114/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0752 - val_loss: 0.1228\n",
            "Epoch 115/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0833 - val_loss: 0.1032\n",
            "Epoch 116/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0922\n",
            "Epoch 117/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0974\n",
            "Epoch 118/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0802 - val_loss: 0.0805\n",
            "Epoch 119/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0755 - val_loss: 0.0855\n",
            "Epoch 120/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0741 - val_loss: 0.1120\n",
            "Epoch 121/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0748 - val_loss: 0.0910\n",
            "Epoch 122/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0770 - val_loss: 0.1463\n",
            "Epoch 123/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.0836\n",
            "Epoch 124/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0775\n",
            "Epoch 125/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0894\n",
            "Epoch 126/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.1623\n",
            "Epoch 127/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0812 - val_loss: 0.0999\n",
            "Epoch 128/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0818 - val_loss: 0.0927\n",
            "Epoch 129/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 0.0787\n",
            "Epoch 130/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.0887\n",
            "Epoch 131/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 0.1107\n",
            "Epoch 132/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0850 - val_loss: 0.0969\n",
            "Epoch 133/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0751 - val_loss: 0.0755\n",
            "Epoch 134/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0796 - val_loss: 0.1590\n",
            "Epoch 135/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0787 - val_loss: 0.0761\n",
            "Epoch 136/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0813 - val_loss: 0.0724\n",
            "Epoch 137/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0760 - val_loss: 0.1019\n",
            "Epoch 138/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0839 - val_loss: 0.1145\n",
            "Epoch 139/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0772 - val_loss: 0.0798\n",
            "Epoch 140/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.0804\n",
            "Epoch 141/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 0.0838\n",
            "Epoch 142/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.0880\n",
            "Epoch 143/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0774\n",
            "Epoch 144/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0748\n",
            "Epoch 145/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0768 - val_loss: 0.1230\n",
            "Epoch 146/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0853 - val_loss: 0.1103\n",
            "Epoch 147/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0758 - val_loss: 0.0753\n",
            "Epoch 148/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 0.0929\n",
            "Epoch 149/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0744 - val_loss: 0.0900\n",
            "Epoch 150/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0756 - val_loss: 0.0727\n",
            "Model with tanh activation, 225 epoch and 0.2 learning rate\n",
            "Epoch 1/225\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.9253 - val_loss: 0.7050\n",
            "Epoch 2/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6027 - val_loss: 0.6131\n",
            "Epoch 3/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5313 - val_loss: 0.5600\n",
            "Epoch 4/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4612 - val_loss: 0.6009\n",
            "Epoch 5/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4430 - val_loss: 0.5467\n",
            "Epoch 6/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3949 - val_loss: 0.7693\n",
            "Epoch 7/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3564 - val_loss: 0.3284\n",
            "Epoch 8/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2696 - val_loss: 0.3263\n",
            "Epoch 9/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2532 - val_loss: 0.2571\n",
            "Epoch 10/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2271 - val_loss: 0.2355\n",
            "Epoch 11/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2149 - val_loss: 0.2262\n",
            "Epoch 12/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1802 - val_loss: 0.2895\n",
            "Epoch 13/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1899 - val_loss: 0.1942\n",
            "Epoch 14/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1591 - val_loss: 0.2648\n",
            "Epoch 15/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1692 - val_loss: 0.1874\n",
            "Epoch 16/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1745 - val_loss: 0.1756\n",
            "Epoch 17/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1631 - val_loss: 0.1766\n",
            "Epoch 18/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1484 - val_loss: 0.1617\n",
            "Epoch 19/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1449 - val_loss: 0.1681\n",
            "Epoch 20/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.1476 - val_loss: 0.1614\n",
            "Epoch 21/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1486 - val_loss: 0.2844\n",
            "Epoch 22/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1446 - val_loss: 0.1527\n",
            "Epoch 23/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1389 - val_loss: 0.1746\n",
            "Epoch 24/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1337 - val_loss: 0.1438\n",
            "Epoch 25/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1418 - val_loss: 0.1512\n",
            "Epoch 26/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1332 - val_loss: 0.1584\n",
            "Epoch 27/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1196 - val_loss: 0.1435\n",
            "Epoch 28/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1151 - val_loss: 0.1475\n",
            "Epoch 29/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1170 - val_loss: 0.1520\n",
            "Epoch 30/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1213 - val_loss: 0.1275\n",
            "Epoch 31/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1282 - val_loss: 0.1333\n",
            "Epoch 32/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.1107 - val_loss: 0.1261\n",
            "Epoch 33/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1106 - val_loss: 0.1751\n",
            "Epoch 34/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1114 - val_loss: 0.1292\n",
            "Epoch 35/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1045 - val_loss: 0.1746\n",
            "Epoch 36/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1115 - val_loss: 0.1332\n",
            "Epoch 37/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0992 - val_loss: 0.1510\n",
            "Epoch 38/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1110 - val_loss: 0.1170\n",
            "Epoch 39/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1058 - val_loss: 0.1370\n",
            "Epoch 40/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1144 - val_loss: 0.1316\n",
            "Epoch 41/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1052 - val_loss: 0.1286\n",
            "Epoch 42/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1069 - val_loss: 0.1148\n",
            "Epoch 43/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1052 - val_loss: 0.1401\n",
            "Epoch 44/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0992 - val_loss: 0.1163\n",
            "Epoch 45/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.1087 - val_loss: 0.1502\n",
            "Epoch 46/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.1073 - val_loss: 0.1138\n",
            "Epoch 47/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1033 - val_loss: 0.1620\n",
            "Epoch 48/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1049 - val_loss: 0.1080\n",
            "Epoch 49/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1006 - val_loss: 0.1269\n",
            "Epoch 50/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0954 - val_loss: 0.1157\n",
            "Epoch 51/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1012 - val_loss: 0.1093\n",
            "Epoch 52/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1044 - val_loss: 0.1436\n",
            "Epoch 53/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1003 - val_loss: 0.1178\n",
            "Epoch 54/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0927 - val_loss: 0.0995\n",
            "Epoch 55/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0948 - val_loss: 0.1073\n",
            "Epoch 56/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1028 - val_loss: 0.1308\n",
            "Epoch 57/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.1419\n",
            "Epoch 58/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0987 - val_loss: 0.1152\n",
            "Epoch 59/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0922 - val_loss: 0.1130\n",
            "Epoch 60/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0923 - val_loss: 0.1006\n",
            "Epoch 61/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0902 - val_loss: 0.1365\n",
            "Epoch 62/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1045 - val_loss: 0.1243\n",
            "Epoch 63/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0884 - val_loss: 0.0916\n",
            "Epoch 64/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0933 - val_loss: 0.1081\n",
            "Epoch 65/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1041 - val_loss: 0.1009\n",
            "Epoch 66/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0845 - val_loss: 0.0887\n",
            "Epoch 67/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1014 - val_loss: 0.1062\n",
            "Epoch 68/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.1016\n",
            "Epoch 69/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0914 - val_loss: 0.0887\n",
            "Epoch 70/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0869 - val_loss: 0.0912\n",
            "Epoch 71/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.1069\n",
            "Epoch 72/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1026 - val_loss: 0.0996\n",
            "Epoch 73/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0826 - val_loss: 0.0995\n",
            "Epoch 74/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0931 - val_loss: 0.1653\n",
            "Epoch 75/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1032 - val_loss: 0.1297\n",
            "Epoch 76/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0904 - val_loss: 0.1161\n",
            "Epoch 77/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0926 - val_loss: 0.1760\n",
            "Epoch 78/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0924 - val_loss: 0.1040\n",
            "Epoch 79/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0857 - val_loss: 0.1020\n",
            "Epoch 80/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0871 - val_loss: 0.1431\n",
            "Epoch 81/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 0.1063\n",
            "Epoch 82/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0917 - val_loss: 0.0882\n",
            "Epoch 83/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0948 - val_loss: 0.0936\n",
            "Epoch 84/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0874 - val_loss: 0.1292\n",
            "Epoch 85/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0889 - val_loss: 0.0932\n",
            "Epoch 86/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0843 - val_loss: 0.0994\n",
            "Epoch 87/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0946 - val_loss: 0.1100\n",
            "Epoch 88/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.1054\n",
            "Epoch 89/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0894 - val_loss: 0.0953\n",
            "Epoch 90/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 0.1670\n",
            "Epoch 91/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0997 - val_loss: 0.0910\n",
            "Epoch 92/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0830 - val_loss: 0.0859\n",
            "Epoch 93/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0826 - val_loss: 0.1105\n",
            "Epoch 94/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 0.1231\n",
            "Epoch 95/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0821 - val_loss: 0.1067\n",
            "Epoch 96/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0760 - val_loss: 0.1035\n",
            "Epoch 97/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 0.1043\n",
            "Epoch 98/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0984 - val_loss: 0.1030\n",
            "Epoch 99/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0851 - val_loss: 0.0881\n",
            "Epoch 100/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0829 - val_loss: 0.1000\n",
            "Epoch 101/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0848 - val_loss: 0.1114\n",
            "Epoch 102/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0848 - val_loss: 0.0899\n",
            "Epoch 103/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0847 - val_loss: 0.1109\n",
            "Epoch 104/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0909 - val_loss: 0.1036\n",
            "Epoch 105/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0840 - val_loss: 0.1514\n",
            "Epoch 106/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0821 - val_loss: 0.1162\n",
            "Epoch 107/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0926 - val_loss: 0.0975\n",
            "Epoch 108/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0811 - val_loss: 0.0871\n",
            "Epoch 109/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.0883\n",
            "Epoch 110/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0768 - val_loss: 0.0943\n",
            "Epoch 111/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0816 - val_loss: 0.0894\n",
            "Epoch 112/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0995 - val_loss: 0.0874\n",
            "Epoch 113/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0742 - val_loss: 0.0924\n",
            "Epoch 114/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0792 - val_loss: 0.1036\n",
            "Epoch 115/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0946 - val_loss: 0.0926\n",
            "Epoch 116/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0822 - val_loss: 0.0885\n",
            "Epoch 117/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0846 - val_loss: 0.1078\n",
            "Epoch 118/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.1321\n",
            "Epoch 119/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0838 - val_loss: 0.1214\n",
            "Epoch 120/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0883 - val_loss: 0.0744\n",
            "Epoch 121/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.0921\n",
            "Epoch 122/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0804 - val_loss: 0.1190\n",
            "Epoch 123/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.1089\n",
            "Epoch 124/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0848 - val_loss: 0.0880\n",
            "Epoch 125/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.0925\n",
            "Epoch 126/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0892 - val_loss: 0.1055\n",
            "Epoch 127/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0834 - val_loss: 0.0854\n",
            "Epoch 128/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0750 - val_loss: 0.1046\n",
            "Epoch 129/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0811 - val_loss: 0.0952\n",
            "Epoch 130/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0861 - val_loss: 0.0763\n",
            "Epoch 131/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0763 - val_loss: 0.0883\n",
            "Epoch 132/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0867 - val_loss: 0.0867\n",
            "Epoch 133/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.0810\n",
            "Epoch 134/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0728 - val_loss: 0.0889\n",
            "Epoch 135/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0983\n",
            "Epoch 136/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0760\n",
            "Epoch 137/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0941 - val_loss: 0.0893\n",
            "Epoch 138/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0737 - val_loss: 0.0860\n",
            "Epoch 139/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0831 - val_loss: 0.1009\n",
            "Epoch 140/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0927 - val_loss: 0.0811\n",
            "Epoch 141/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0852\n",
            "Epoch 142/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0746\n",
            "Epoch 143/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0752 - val_loss: 0.0882\n",
            "Epoch 144/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0787 - val_loss: 0.1149\n",
            "Epoch 145/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0834 - val_loss: 0.1084\n",
            "Epoch 146/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0857 - val_loss: 0.0916\n",
            "Epoch 147/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0762\n",
            "Epoch 148/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0730 - val_loss: 0.0877\n",
            "Epoch 149/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0754 - val_loss: 0.0864\n",
            "Epoch 150/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0794 - val_loss: 0.0869\n",
            "Epoch 151/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0828 - val_loss: 0.1065\n",
            "Epoch 152/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0780 - val_loss: 0.0942\n",
            "Epoch 153/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0739 - val_loss: 0.0831\n",
            "Epoch 154/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0755 - val_loss: 0.1002\n",
            "Epoch 155/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0847 - val_loss: 0.1267\n",
            "Epoch 156/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0738 - val_loss: 0.0756\n",
            "Epoch 157/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0836 - val_loss: 0.0850\n",
            "Epoch 158/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0771\n",
            "Epoch 159/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0799 - val_loss: 0.1077\n",
            "Epoch 160/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0778 - val_loss: 0.0989\n",
            "Epoch 161/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0815 - val_loss: 0.0846\n",
            "Epoch 162/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0824 - val_loss: 0.0746\n",
            "Epoch 163/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0770 - val_loss: 0.1048\n",
            "Epoch 164/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0840 - val_loss: 0.0932\n",
            "Epoch 165/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0690 - val_loss: 0.0887\n",
            "Epoch 166/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0918 - val_loss: 0.0922\n",
            "Epoch 167/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0682 - val_loss: 0.1082\n",
            "Epoch 168/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 0.0781\n",
            "Epoch 169/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0796 - val_loss: 0.0883\n",
            "Epoch 170/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.0854\n",
            "Epoch 171/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0753\n",
            "Epoch 172/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0696 - val_loss: 0.1110\n",
            "Epoch 173/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 0.0956\n",
            "Epoch 174/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0868 - val_loss: 0.1066\n",
            "Epoch 175/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0749 - val_loss: 0.0764\n",
            "Epoch 176/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0774 - val_loss: 0.0780\n",
            "Epoch 177/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0873\n",
            "Epoch 178/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0748 - val_loss: 0.0733\n",
            "Epoch 179/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0761 - val_loss: 0.1063\n",
            "Epoch 180/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0796\n",
            "Epoch 181/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0710 - val_loss: 0.0746\n",
            "Epoch 182/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0707 - val_loss: 0.1571\n",
            "Epoch 183/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 0.0765\n",
            "Epoch 184/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0869\n",
            "Epoch 185/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0840 - val_loss: 0.0767\n",
            "Epoch 186/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0975\n",
            "Epoch 187/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0765 - val_loss: 0.1599\n",
            "Epoch 188/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 0.0892\n",
            "Epoch 189/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0744 - val_loss: 0.1063\n",
            "Epoch 190/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0704 - val_loss: 0.0952\n",
            "Epoch 191/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0753 - val_loss: 0.0785\n",
            "Epoch 192/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0718\n",
            "Epoch 193/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.0911\n",
            "Epoch 194/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0887\n",
            "Epoch 195/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0764 - val_loss: 0.0736\n",
            "Epoch 196/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0785 - val_loss: 0.0774\n",
            "Epoch 197/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.0910\n",
            "Epoch 198/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
            "Epoch 199/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.1057\n",
            "Epoch 200/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0706 - val_loss: 0.0886\n",
            "Epoch 201/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0705 - val_loss: 0.1005\n",
            "Epoch 202/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0784 - val_loss: 0.0724\n",
            "Epoch 203/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0749 - val_loss: 0.0964\n",
            "Epoch 204/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0762 - val_loss: 0.0685\n",
            "Epoch 205/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0765 - val_loss: 0.0822\n",
            "Epoch 206/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0729 - val_loss: 0.0898\n",
            "Epoch 207/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0805\n",
            "Epoch 208/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.0864\n",
            "Epoch 209/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0818 - val_loss: 0.0887\n",
            "Epoch 210/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.0700\n",
            "Epoch 211/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0840\n",
            "Epoch 212/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0701 - val_loss: 0.0784\n",
            "Epoch 213/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0819 - val_loss: 0.0764\n",
            "Epoch 214/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0745 - val_loss: 0.0706\n",
            "Epoch 215/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0706 - val_loss: 0.0710\n",
            "Epoch 216/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0677 - val_loss: 0.0751\n",
            "Epoch 217/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0787 - val_loss: 0.1110\n",
            "Epoch 218/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0991\n",
            "Epoch 219/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0718 - val_loss: 0.1165\n",
            "Epoch 220/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0737 - val_loss: 0.0824\n",
            "Epoch 221/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0791 - val_loss: 0.0749\n",
            "Epoch 222/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0700 - val_loss: 0.0678\n",
            "Epoch 223/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0954\n",
            "Epoch 224/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0703 - val_loss: 0.0798\n",
            "Epoch 225/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0727 - val_loss: 0.1090\n",
            "Model with sigmoid activation, 100 epoch and 0.05 learning rate\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 2.0033 - val_loss: 0.9680\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7738 - val_loss: 0.7517\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7052 - val_loss: 0.7420\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7030 - val_loss: 0.7420\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7028 - val_loss: 0.7419\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 0.7424\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7026 - val_loss: 0.7410\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 0.7409\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 0.7430\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7027 - val_loss: 0.7422\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7026 - val_loss: 0.7413\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7027 - val_loss: 0.7407\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7026 - val_loss: 0.7413\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7026 - val_loss: 0.7411\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7023 - val_loss: 0.7405\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7027 - val_loss: 0.7411\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7020 - val_loss: 0.7412\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7025 - val_loss: 0.7411\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7021 - val_loss: 0.7416\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7026 - val_loss: 0.7402\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7021 - val_loss: 0.7407\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7022 - val_loss: 0.7409\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7020 - val_loss: 0.7414\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7020 - val_loss: 0.7406\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7016 - val_loss: 0.7401\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7018 - val_loss: 0.7421\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7021 - val_loss: 0.7411\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7413\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7415\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7407\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7016 - val_loss: 0.7407\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7014 - val_loss: 0.7400\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7420\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7402\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7412\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7015 - val_loss: 0.7398\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7014 - val_loss: 0.7399\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7409\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7395\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7403\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7404\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7412\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7394\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7005 - val_loss: 0.7389\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 0.7394\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 0.7385\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 0.7400\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7005 - val_loss: 0.7389\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7005 - val_loss: 0.7392\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7387\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7004 - val_loss: 0.7383\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7383\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7001 - val_loss: 0.7393\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7000 - val_loss: 0.7391\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7001 - val_loss: 0.7387\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6997 - val_loss: 0.7377\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6998 - val_loss: 0.7383\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.7406\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6996 - val_loss: 0.7391\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6991 - val_loss: 0.7372\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6993 - val_loss: 0.7369\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7377\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6991 - val_loss: 0.7370\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6991 - val_loss: 0.7364\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.7377\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6990 - val_loss: 0.7382\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6990 - val_loss: 0.7368\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6986 - val_loss: 0.7370\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6988 - val_loss: 0.7361\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6983 - val_loss: 0.7359\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6983 - val_loss: 0.7373\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6982 - val_loss: 0.7371\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6979 - val_loss: 0.7371\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6979 - val_loss: 0.7366\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6979 - val_loss: 0.7359\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6977 - val_loss: 0.7374\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - val_loss: 0.7366\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6974 - val_loss: 0.7352\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6971 - val_loss: 0.7363\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6974 - val_loss: 0.7362\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6966 - val_loss: 0.7349\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 0.7354\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6966 - val_loss: 0.7344\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6965 - val_loss: 0.7360\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6963 - val_loss: 0.7353\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6961 - val_loss: 0.7346\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6956 - val_loss: 0.7345\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6957 - val_loss: 0.7343\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 0.7338\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 0.7335\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6951 - val_loss: 0.7330\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6948 - val_loss: 0.7343\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6948 - val_loss: 0.7337\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 0.7344\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 0.7331\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6944 - val_loss: 0.7328\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6940 - val_loss: 0.7317\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 0.7320\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 0.7319\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - val_loss: 0.7327\n",
            "Model with sigmoid activation, 150 epoch and 0.05 learning rate\n",
            "Epoch 1/150\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 2.0092 - val_loss: 0.9819\n",
            "Epoch 2/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7784 - val_loss: 0.7579\n",
            "Epoch 3/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7095 - val_loss: 0.7450\n",
            "Epoch 4/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7066 - val_loss: 0.7462\n",
            "Epoch 5/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7065 - val_loss: 0.7453\n",
            "Epoch 6/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7066 - val_loss: 0.7448\n",
            "Epoch 7/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 0.7443\n",
            "Epoch 8/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7065 - val_loss: 0.7451\n",
            "Epoch 9/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7062 - val_loss: 0.7441\n",
            "Epoch 10/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7061 - val_loss: 0.7476\n",
            "Epoch 11/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7064 - val_loss: 0.7451\n",
            "Epoch 12/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 0.7450\n",
            "Epoch 13/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7060 - val_loss: 0.7437\n",
            "Epoch 14/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7057 - val_loss: 0.7441\n",
            "Epoch 15/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7059 - val_loss: 0.7448\n",
            "Epoch 16/150\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7058 - val_loss: 0.7435\n",
            "Epoch 17/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 0.7435\n",
            "Epoch 18/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7056 - val_loss: 0.7446\n",
            "Epoch 19/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7053 - val_loss: 0.7442\n",
            "Epoch 20/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7056 - val_loss: 0.7425\n",
            "Epoch 21/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7055 - val_loss: 0.7444\n",
            "Epoch 22/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7055 - val_loss: 0.7431\n",
            "Epoch 23/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7055 - val_loss: 0.7447\n",
            "Epoch 24/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7051 - val_loss: 0.7431\n",
            "Epoch 25/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7049 - val_loss: 0.7421\n",
            "Epoch 26/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7051 - val_loss: 0.7426\n",
            "Epoch 27/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7052 - val_loss: 0.7422\n",
            "Epoch 28/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7053 - val_loss: 0.7434\n",
            "Epoch 29/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7050 - val_loss: 0.7437\n",
            "Epoch 30/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7049 - val_loss: 0.7439\n",
            "Epoch 31/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7050 - val_loss: 0.7430\n",
            "Epoch 32/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7047 - val_loss: 0.7424\n",
            "Epoch 33/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7047 - val_loss: 0.7437\n",
            "Epoch 34/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7048 - val_loss: 0.7424\n",
            "Epoch 35/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 0.7423\n",
            "Epoch 36/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7435\n",
            "Epoch 37/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7045 - val_loss: 0.7430\n",
            "Epoch 38/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7421\n",
            "Epoch 39/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7045 - val_loss: 0.7434\n",
            "Epoch 40/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 0.7425\n",
            "Epoch 41/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7425\n",
            "Epoch 42/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7434\n",
            "Epoch 43/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7417\n",
            "Epoch 44/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7426\n",
            "Epoch 45/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7423\n",
            "Epoch 46/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7418\n",
            "Epoch 47/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7041 - val_loss: 0.7421\n",
            "Epoch 48/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7411\n",
            "Epoch 49/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7428\n",
            "Epoch 50/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7038 - val_loss: 0.7432\n",
            "Epoch 51/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7411\n",
            "Epoch 52/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.7418\n",
            "Epoch 53/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7415\n",
            "Epoch 54/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7037 - val_loss: 0.7410\n",
            "Epoch 55/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7409\n",
            "Epoch 56/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7037 - val_loss: 0.7416\n",
            "Epoch 57/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7404\n",
            "Epoch 58/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 0.7416\n",
            "Epoch 59/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7032 - val_loss: 0.7420\n",
            "Epoch 60/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7419\n",
            "Epoch 61/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7410\n",
            "Epoch 62/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7418\n",
            "Epoch 63/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7419\n",
            "Epoch 64/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7405\n",
            "Epoch 65/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7029 - val_loss: 0.7405\n",
            "Epoch 66/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7396\n",
            "Epoch 67/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7400\n",
            "Epoch 68/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7402\n",
            "Epoch 69/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7025 - val_loss: 0.7422\n",
            "Epoch 70/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7026 - val_loss: 0.7414\n",
            "Epoch 71/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7407\n",
            "Epoch 72/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7029 - val_loss: 0.7400\n",
            "Epoch 73/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.7405\n",
            "Epoch 74/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7021 - val_loss: 0.7394\n",
            "Epoch 75/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7420\n",
            "Epoch 76/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7024 - val_loss: 0.7395\n",
            "Epoch 77/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7023 - val_loss: 0.7398\n",
            "Epoch 78/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7017 - val_loss: 0.7396\n",
            "Epoch 79/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7019 - val_loss: 0.7397\n",
            "Epoch 80/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7023 - val_loss: 0.7390\n",
            "Epoch 81/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7021 - val_loss: 0.7393\n",
            "Epoch 82/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7018 - val_loss: 0.7394\n",
            "Epoch 83/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7019 - val_loss: 0.7399\n",
            "Epoch 84/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7387\n",
            "Epoch 85/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7396\n",
            "Epoch 86/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7390\n",
            "Epoch 87/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7014 - val_loss: 0.7390\n",
            "Epoch 88/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7396\n",
            "Epoch 89/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.7394\n",
            "Epoch 90/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7014 - val_loss: 0.7396\n",
            "Epoch 91/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7011 - val_loss: 0.7397\n",
            "Epoch 92/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7010 - val_loss: 0.7383\n",
            "Epoch 93/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7390\n",
            "Epoch 94/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7389\n",
            "Epoch 95/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7391\n",
            "Epoch 96/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7402\n",
            "Epoch 97/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7389\n",
            "Epoch 98/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7384\n",
            "Epoch 99/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7001 - val_loss: 0.7390\n",
            "Epoch 100/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7387\n",
            "Epoch 101/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7001 - val_loss: 0.7379\n",
            "Epoch 102/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7003 - val_loss: 0.7378\n",
            "Epoch 103/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7382\n",
            "Epoch 104/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7383\n",
            "Epoch 105/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6997 - val_loss: 0.7382\n",
            "Epoch 106/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6997 - val_loss: 0.7380\n",
            "Epoch 107/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6998 - val_loss: 0.7367\n",
            "Epoch 108/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6997 - val_loss: 0.7366\n",
            "Epoch 109/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6995 - val_loss: 0.7373\n",
            "Epoch 110/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6997 - val_loss: 0.7374\n",
            "Epoch 111/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6993 - val_loss: 0.7381\n",
            "Epoch 112/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6996 - val_loss: 0.7372\n",
            "Epoch 113/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6993 - val_loss: 0.7367\n",
            "Epoch 114/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6992 - val_loss: 0.7366\n",
            "Epoch 115/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6991 - val_loss: 0.7364\n",
            "Epoch 116/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6986 - val_loss: 0.7371\n",
            "Epoch 117/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6987 - val_loss: 0.7353\n",
            "Epoch 118/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6985 - val_loss: 0.7364\n",
            "Epoch 119/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6985 - val_loss: 0.7372\n",
            "Epoch 120/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6986 - val_loss: 0.7359\n",
            "Epoch 121/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6981 - val_loss: 0.7357\n",
            "Epoch 122/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6979 - val_loss: 0.7359\n",
            "Epoch 123/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6981 - val_loss: 0.7350\n",
            "Epoch 124/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 0.7357\n",
            "Epoch 125/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6975 - val_loss: 0.7358\n",
            "Epoch 126/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6977 - val_loss: 0.7346\n",
            "Epoch 127/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6976 - val_loss: 0.7359\n",
            "Epoch 128/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6973 - val_loss: 0.7351\n",
            "Epoch 129/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6970 - val_loss: 0.7347\n",
            "Epoch 130/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6971 - val_loss: 0.7347\n",
            "Epoch 131/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6967 - val_loss: 0.7352\n",
            "Epoch 132/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6970 - val_loss: 0.7347\n",
            "Epoch 133/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6964 - val_loss: 0.7342\n",
            "Epoch 134/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6959 - val_loss: 0.7336\n",
            "Epoch 135/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6961 - val_loss: 0.7326\n",
            "Epoch 136/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6964 - val_loss: 0.7326\n",
            "Epoch 137/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6960 - val_loss: 0.7333\n",
            "Epoch 138/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6957 - val_loss: 0.7332\n",
            "Epoch 139/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6953 - val_loss: 0.7327\n",
            "Epoch 140/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6951 - val_loss: 0.7321\n",
            "Epoch 141/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6949 - val_loss: 0.7329\n",
            "Epoch 142/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6949 - val_loss: 0.7338\n",
            "Epoch 143/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6948 - val_loss: 0.7311\n",
            "Epoch 144/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 0.7312\n",
            "Epoch 145/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6942 - val_loss: 0.7323\n",
            "Epoch 146/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6942 - val_loss: 0.7312\n",
            "Epoch 147/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 0.7302\n",
            "Epoch 148/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6937 - val_loss: 0.7302\n",
            "Epoch 149/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - val_loss: 0.7293\n",
            "Epoch 150/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.7300\n",
            "Model with sigmoid activation, 225 epoch and 0.05 learning rate\n",
            "Epoch 1/225\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 1.6189 - val_loss: 0.8381\n",
            "Epoch 2/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7278 - val_loss: 0.7465\n",
            "Epoch 3/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7048 - val_loss: 0.7425\n",
            "Epoch 4/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7045 - val_loss: 0.7440\n",
            "Epoch 5/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 0.7454\n",
            "Epoch 6/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7445\n",
            "Epoch 7/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 0.7446\n",
            "Epoch 8/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7043 - val_loss: 0.7442\n",
            "Epoch 9/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7440\n",
            "Epoch 10/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7427\n",
            "Epoch 11/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7042 - val_loss: 0.7420\n",
            "Epoch 12/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7435\n",
            "Epoch 13/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7040 - val_loss: 0.7443\n",
            "Epoch 14/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7426\n",
            "Epoch 15/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7426\n",
            "Epoch 16/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7043 - val_loss: 0.7430\n",
            "Epoch 17/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7435\n",
            "Epoch 18/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7434\n",
            "Epoch 19/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7038 - val_loss: 0.7434\n",
            "Epoch 20/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7435\n",
            "Epoch 21/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7435\n",
            "Epoch 22/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7424\n",
            "Epoch 23/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7431\n",
            "Epoch 24/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7036 - val_loss: 0.7426\n",
            "Epoch 25/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7414\n",
            "Epoch 26/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7039 - val_loss: 0.7432\n",
            "Epoch 27/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.7418\n",
            "Epoch 28/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7426\n",
            "Epoch 29/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7421\n",
            "Epoch 30/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7415\n",
            "Epoch 31/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.7421\n",
            "Epoch 32/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7038 - val_loss: 0.7433\n",
            "Epoch 33/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7037 - val_loss: 0.7427\n",
            "Epoch 34/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 0.7435\n",
            "Epoch 35/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7034 - val_loss: 0.7424\n",
            "Epoch 36/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7036 - val_loss: 0.7440\n",
            "Epoch 37/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7426\n",
            "Epoch 38/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7036 - val_loss: 0.7427\n",
            "Epoch 39/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7425\n",
            "Epoch 40/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7437\n",
            "Epoch 41/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7427\n",
            "Epoch 42/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7429\n",
            "Epoch 43/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7421\n",
            "Epoch 44/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7418\n",
            "Epoch 45/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 0.7426\n",
            "Epoch 46/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7424\n",
            "Epoch 47/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 0.7423\n",
            "Epoch 48/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 0.7432\n",
            "Epoch 49/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7436\n",
            "Epoch 50/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7032 - val_loss: 0.7428\n",
            "Epoch 51/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7032 - val_loss: 0.7431\n",
            "Epoch 52/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7421\n",
            "Epoch 53/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7026 - val_loss: 0.7423\n",
            "Epoch 54/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7413\n",
            "Epoch 55/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7430\n",
            "Epoch 56/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7031 - val_loss: 0.7429\n",
            "Epoch 57/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7416\n",
            "Epoch 58/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7031 - val_loss: 0.7432\n",
            "Epoch 59/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 0.7426\n",
            "Epoch 60/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7027 - val_loss: 0.7430\n",
            "Epoch 61/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7435\n",
            "Epoch 62/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 0.7424\n",
            "Epoch 63/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7024 - val_loss: 0.7412\n",
            "Epoch 64/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7421\n",
            "Epoch 65/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7418\n",
            "Epoch 66/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.7423\n",
            "Epoch 67/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 0.7415\n",
            "Epoch 68/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7026 - val_loss: 0.7416\n",
            "Epoch 69/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7404\n",
            "Epoch 70/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7422\n",
            "Epoch 71/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7026 - val_loss: 0.7420\n",
            "Epoch 72/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7023 - val_loss: 0.7417\n",
            "Epoch 73/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7026 - val_loss: 0.7415\n",
            "Epoch 74/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7025 - val_loss: 0.7404\n",
            "Epoch 75/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7024 - val_loss: 0.7416\n",
            "Epoch 76/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.7426\n",
            "Epoch 77/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7025 - val_loss: 0.7402\n",
            "Epoch 78/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7022 - val_loss: 0.7417\n",
            "Epoch 79/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7024 - val_loss: 0.7408\n",
            "Epoch 80/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7025 - val_loss: 0.7417\n",
            "Epoch 81/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7020 - val_loss: 0.7436\n",
            "Epoch 82/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7024 - val_loss: 0.7412\n",
            "Epoch 83/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7021 - val_loss: 0.7402\n",
            "Epoch 84/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.7418\n",
            "Epoch 85/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7421\n",
            "Epoch 86/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7398\n",
            "Epoch 87/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7413\n",
            "Epoch 88/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7020 - val_loss: 0.7402\n",
            "Epoch 89/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7410\n",
            "Epoch 90/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.7397\n",
            "Epoch 91/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7414\n",
            "Epoch 92/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7016 - val_loss: 0.7421\n",
            "Epoch 93/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7411\n",
            "Epoch 94/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7015 - val_loss: 0.7406\n",
            "Epoch 95/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7015 - val_loss: 0.7412\n",
            "Epoch 96/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7409\n",
            "Epoch 97/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7015 - val_loss: 0.7427\n",
            "Epoch 98/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7399\n",
            "Epoch 99/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7009 - val_loss: 0.7419\n",
            "Epoch 100/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7015 - val_loss: 0.7401\n",
            "Epoch 101/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7404\n",
            "Epoch 102/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7401\n",
            "Epoch 103/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7395\n",
            "Epoch 104/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7398\n",
            "Epoch 105/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7011 - val_loss: 0.7404\n",
            "Epoch 106/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7405\n",
            "Epoch 107/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7406\n",
            "Epoch 108/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7395\n",
            "Epoch 109/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7400\n",
            "Epoch 110/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7392\n",
            "Epoch 111/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7391\n",
            "Epoch 112/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7381\n",
            "Epoch 113/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7005 - val_loss: 0.7392\n",
            "Epoch 114/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7006 - val_loss: 0.7407\n",
            "Epoch 115/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7007 - val_loss: 0.7399\n",
            "Epoch 116/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7005 - val_loss: 0.7402\n",
            "Epoch 117/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7005 - val_loss: 0.7390\n",
            "Epoch 118/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7006 - val_loss: 0.7392\n",
            "Epoch 119/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7001 - val_loss: 0.7392\n",
            "Epoch 120/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7000 - val_loss: 0.7391\n",
            "Epoch 121/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7003 - val_loss: 0.7399\n",
            "Epoch 122/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7000 - val_loss: 0.7400\n",
            "Epoch 123/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6999 - val_loss: 0.7379\n",
            "Epoch 124/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6997 - val_loss: 0.7390\n",
            "Epoch 125/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6998 - val_loss: 0.7389\n",
            "Epoch 126/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.7385\n",
            "Epoch 127/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6996 - val_loss: 0.7388\n",
            "Epoch 128/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7396\n",
            "Epoch 129/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.7387\n",
            "Epoch 130/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.7390\n",
            "Epoch 131/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6995 - val_loss: 0.7388\n",
            "Epoch 132/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6991 - val_loss: 0.7380\n",
            "Epoch 133/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.7381\n",
            "Epoch 134/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.7388\n",
            "Epoch 135/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6989 - val_loss: 0.7411\n",
            "Epoch 136/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.7376\n",
            "Epoch 137/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6986 - val_loss: 0.7366\n",
            "Epoch 138/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6988 - val_loss: 0.7377\n",
            "Epoch 139/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6988 - val_loss: 0.7381\n",
            "Epoch 140/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6981 - val_loss: 0.7402\n",
            "Epoch 141/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6988 - val_loss: 0.7366\n",
            "Epoch 142/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6982 - val_loss: 0.7377\n",
            "Epoch 143/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6983 - val_loss: 0.7376\n",
            "Epoch 144/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6982 - val_loss: 0.7386\n",
            "Epoch 145/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 0.7383\n",
            "Epoch 146/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6979 - val_loss: 0.7379\n",
            "Epoch 147/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 0.7363\n",
            "Epoch 148/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6979 - val_loss: 0.7371\n",
            "Epoch 149/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6974 - val_loss: 0.7356\n",
            "Epoch 150/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6974 - val_loss: 0.7369\n",
            "Epoch 151/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6970 - val_loss: 0.7357\n",
            "Epoch 152/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6973 - val_loss: 0.7366\n",
            "Epoch 153/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6969 - val_loss: 0.7372\n",
            "Epoch 154/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6970 - val_loss: 0.7365\n",
            "Epoch 155/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 0.7363\n",
            "Epoch 156/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6965 - val_loss: 0.7355\n",
            "Epoch 157/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6965 - val_loss: 0.7351\n",
            "Epoch 158/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6961 - val_loss: 0.7350\n",
            "Epoch 159/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6965 - val_loss: 0.7346\n",
            "Epoch 160/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6962 - val_loss: 0.7350\n",
            "Epoch 161/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6957 - val_loss: 0.7348\n",
            "Epoch 162/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6956 - val_loss: 0.7342\n",
            "Epoch 163/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6955 - val_loss: 0.7365\n",
            "Epoch 164/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6956 - val_loss: 0.7337\n",
            "Epoch 165/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6956 - val_loss: 0.7334\n",
            "Epoch 166/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6952 - val_loss: 0.7352\n",
            "Epoch 167/225\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6948 - val_loss: 0.7345\n",
            "Epoch 168/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6946 - val_loss: 0.7346\n",
            "Epoch 169/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6945 - val_loss: 0.7337\n",
            "Epoch 170/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6945 - val_loss: 0.7339\n",
            "Epoch 171/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6942 - val_loss: 0.7341\n",
            "Epoch 172/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 0.7324\n",
            "Epoch 173/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6942 - val_loss: 0.7354\n",
            "Epoch 174/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6937 - val_loss: 0.7351\n",
            "Epoch 175/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6938 - val_loss: 0.7316\n",
            "Epoch 176/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6933 - val_loss: 0.7314\n",
            "Epoch 177/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - val_loss: 0.7319\n",
            "Epoch 178/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6927 - val_loss: 0.7318\n",
            "Epoch 179/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6924 - val_loss: 0.7317\n",
            "Epoch 180/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6923 - val_loss: 0.7317\n",
            "Epoch 181/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6923 - val_loss: 0.7301\n",
            "Epoch 182/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6922 - val_loss: 0.7313\n",
            "Epoch 183/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6918 - val_loss: 0.7316\n",
            "Epoch 184/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6914 - val_loss: 0.7316\n",
            "Epoch 185/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6915 - val_loss: 0.7295\n",
            "Epoch 186/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - val_loss: 0.7290\n",
            "Epoch 187/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6906 - val_loss: 0.7296\n",
            "Epoch 188/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - val_loss: 0.7293\n",
            "Epoch 189/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6903 - val_loss: 0.7292\n",
            "Epoch 190/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6896 - val_loss: 0.7291\n",
            "Epoch 191/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6893 - val_loss: 0.7289\n",
            "Epoch 192/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6891 - val_loss: 0.7290\n",
            "Epoch 193/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6889 - val_loss: 0.7274\n",
            "Epoch 194/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6882 - val_loss: 0.7281\n",
            "Epoch 195/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - val_loss: 0.7253\n",
            "Epoch 196/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - val_loss: 0.7273\n",
            "Epoch 197/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6877 - val_loss: 0.7258\n",
            "Epoch 198/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6872 - val_loss: 0.7271\n",
            "Epoch 199/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6867 - val_loss: 0.7259\n",
            "Epoch 200/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6856 - val_loss: 0.7245\n",
            "Epoch 201/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6860 - val_loss: 0.7238\n",
            "Epoch 202/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6853 - val_loss: 0.7250\n",
            "Epoch 203/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6851 - val_loss: 0.7229\n",
            "Epoch 204/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6840 - val_loss: 0.7245\n",
            "Epoch 205/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6844 - val_loss: 0.7218\n",
            "Epoch 206/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6836 - val_loss: 0.7218\n",
            "Epoch 207/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6829 - val_loss: 0.7221\n",
            "Epoch 208/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6824 - val_loss: 0.7210\n",
            "Epoch 209/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6821 - val_loss: 0.7194\n",
            "Epoch 210/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6814 - val_loss: 0.7194\n",
            "Epoch 211/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6807 - val_loss: 0.7198\n",
            "Epoch 212/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6803 - val_loss: 0.7197\n",
            "Epoch 213/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6797 - val_loss: 0.7222\n",
            "Epoch 214/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.7180\n",
            "Epoch 215/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6783 - val_loss: 0.7170\n",
            "Epoch 216/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6777 - val_loss: 0.7165\n",
            "Epoch 217/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6772 - val_loss: 0.7156\n",
            "Epoch 218/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6763 - val_loss: 0.7162\n",
            "Epoch 219/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6756 - val_loss: 0.7148\n",
            "Epoch 220/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6749 - val_loss: 0.7145\n",
            "Epoch 221/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6743 - val_loss: 0.7140\n",
            "Epoch 222/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6732 - val_loss: 0.7127\n",
            "Epoch 223/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6727 - val_loss: 0.7108\n",
            "Epoch 224/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6716 - val_loss: 0.7102\n",
            "Epoch 225/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6703 - val_loss: 0.7091\n",
            "Model with sigmoid activation, 100 epoch and 0.1 learning rate\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 1.4625 - val_loss: 0.7551\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7069 - val_loss: 0.7440\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7050 - val_loss: 0.7434\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7049 - val_loss: 0.7432\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7058 - val_loss: 0.7425\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7050 - val_loss: 0.7443\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 0.7484\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7056 - val_loss: 0.7429\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7049 - val_loss: 0.7432\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7413\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7046 - val_loss: 0.7451\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7045 - val_loss: 0.7410\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7423\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7440\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7425\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7419\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7034 - val_loss: 0.7478\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7434\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.7410\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7035 - val_loss: 0.7424\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7426\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7415\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7034 - val_loss: 0.7442\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7030 - val_loss: 0.7429\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7033 - val_loss: 0.7409\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7033 - val_loss: 0.7405\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7406\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7032 - val_loss: 0.7400\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.7406\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.7435\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7029 - val_loss: 0.7409\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7024 - val_loss: 0.7407\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7021 - val_loss: 0.7416\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7021 - val_loss: 0.7380\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7023 - val_loss: 0.7391\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.7418\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.7400\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7013 - val_loss: 0.7414\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7420\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7385\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7405\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7379\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7010 - val_loss: 0.7399\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7375\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7006 - val_loss: 0.7393\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7008 - val_loss: 0.7397\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7006 - val_loss: 0.7407\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7374\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7002 - val_loss: 0.7368\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6999 - val_loss: 0.7377\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6995 - val_loss: 0.7371\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6992 - val_loss: 0.7391\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6998 - val_loss: 0.7384\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6998 - val_loss: 0.7359\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6985 - val_loss: 0.7390\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6987 - val_loss: 0.7371\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6989 - val_loss: 0.7389\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6987 - val_loss: 0.7369\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6983 - val_loss: 0.7351\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6976 - val_loss: 0.7354\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6972 - val_loss: 0.7376\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6980 - val_loss: 0.7343\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6971 - val_loss: 0.7367\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6973 - val_loss: 0.7363\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6970 - val_loss: 0.7353\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6961 - val_loss: 0.7339\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6957 - val_loss: 0.7342\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6954 - val_loss: 0.7319\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6958 - val_loss: 0.7324\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6954 - val_loss: 0.7351\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6949 - val_loss: 0.7335\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6944 - val_loss: 0.7321\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6940 - val_loss: 0.7342\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6939 - val_loss: 0.7328\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6937 - val_loss: 0.7308\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.7309\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6923 - val_loss: 0.7301\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6917 - val_loss: 0.7294\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6915 - val_loss: 0.7291\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6907 - val_loss: 0.7282\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6903 - val_loss: 0.7265\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6902 - val_loss: 0.7296\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6892 - val_loss: 0.7283\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6883 - val_loss: 0.7274\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6879 - val_loss: 0.7254\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6866 - val_loss: 0.7264\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6857 - val_loss: 0.7241\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6855 - val_loss: 0.7220\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6846 - val_loss: 0.7251\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6842 - val_loss: 0.7223\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6829 - val_loss: 0.7230\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6812 - val_loss: 0.7217\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6808 - val_loss: 0.7197\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6797 - val_loss: 0.7180\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6785 - val_loss: 0.7182\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6774 - val_loss: 0.7155\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6758 - val_loss: 0.7146\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6741 - val_loss: 0.7124\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6724 - val_loss: 0.7086\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 0.7084\n",
            "Model with sigmoid activation, 150 epoch and 0.1 learning rate\n",
            "Epoch 1/150\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 1.4124 - val_loss: 0.7481\n",
            "Epoch 2/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7051 - val_loss: 0.7417\n",
            "Epoch 3/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7054 - val_loss: 0.7466\n",
            "Epoch 4/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7051 - val_loss: 0.7422\n",
            "Epoch 5/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7046 - val_loss: 0.7445\n",
            "Epoch 6/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7047 - val_loss: 0.7433\n",
            "Epoch 7/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7461\n",
            "Epoch 8/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7440\n",
            "Epoch 9/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7051 - val_loss: 0.7450\n",
            "Epoch 10/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7049 - val_loss: 0.7443\n",
            "Epoch 11/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7043 - val_loss: 0.7469\n",
            "Epoch 12/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.7426\n",
            "Epoch 13/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7043 - val_loss: 0.7417\n",
            "Epoch 14/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7416\n",
            "Epoch 15/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7043 - val_loss: 0.7426\n",
            "Epoch 16/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7436\n",
            "Epoch 17/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7433\n",
            "Epoch 18/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7039 - val_loss: 0.7409\n",
            "Epoch 19/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7434\n",
            "Epoch 20/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7423\n",
            "Epoch 21/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7046 - val_loss: 0.7425\n",
            "Epoch 22/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7436\n",
            "Epoch 23/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7038 - val_loss: 0.7428\n",
            "Epoch 24/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7039 - val_loss: 0.7427\n",
            "Epoch 25/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.7430\n",
            "Epoch 26/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7039 - val_loss: 0.7426\n",
            "Epoch 27/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7444\n",
            "Epoch 28/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.7425\n",
            "Epoch 29/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7421\n",
            "Epoch 30/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7032 - val_loss: 0.7490\n",
            "Epoch 31/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7038 - val_loss: 0.7434\n",
            "Epoch 32/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.7406\n",
            "Epoch 33/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7423\n",
            "Epoch 34/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7396\n",
            "Epoch 35/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7439\n",
            "Epoch 36/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7479\n",
            "Epoch 37/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7403\n",
            "Epoch 38/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7415\n",
            "Epoch 39/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7024 - val_loss: 0.7395\n",
            "Epoch 40/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7415\n",
            "Epoch 41/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.7419\n",
            "Epoch 42/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.7417\n",
            "Epoch 43/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7396\n",
            "Epoch 44/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.7423\n",
            "Epoch 45/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.7397\n",
            "Epoch 46/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7020 - val_loss: 0.7395\n",
            "Epoch 47/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7423\n",
            "Epoch 48/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7400\n",
            "Epoch 49/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7015 - val_loss: 0.7411\n",
            "Epoch 50/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 0.7441\n",
            "Epoch 51/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7022 - val_loss: 0.7385\n",
            "Epoch 52/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7441\n",
            "Epoch 53/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7012 - val_loss: 0.7381\n",
            "Epoch 54/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7019 - val_loss: 0.7391\n",
            "Epoch 55/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7007 - val_loss: 0.7394\n",
            "Epoch 56/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.7393\n",
            "Epoch 57/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7014 - val_loss: 0.7374\n",
            "Epoch 58/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7017 - val_loss: 0.7412\n",
            "Epoch 59/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7009 - val_loss: 0.7363\n",
            "Epoch 60/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7005 - val_loss: 0.7411\n",
            "Epoch 61/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7004 - val_loss: 0.7406\n",
            "Epoch 62/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7008 - val_loss: 0.7390\n",
            "Epoch 63/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.7384\n",
            "Epoch 64/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7004 - val_loss: 0.7383\n",
            "Epoch 65/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6998 - val_loss: 0.7372\n",
            "Epoch 66/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6999 - val_loss: 0.7424\n",
            "Epoch 67/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7383\n",
            "Epoch 68/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7001 - val_loss: 0.7387\n",
            "Epoch 69/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6997 - val_loss: 0.7377\n",
            "Epoch 70/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6996 - val_loss: 0.7388\n",
            "Epoch 71/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6996 - val_loss: 0.7378\n",
            "Epoch 72/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6997 - val_loss: 0.7394\n",
            "Epoch 73/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6986 - val_loss: 0.7391\n",
            "Epoch 74/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6995 - val_loss: 0.7387\n",
            "Epoch 75/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6985 - val_loss: 0.7377\n",
            "Epoch 76/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6990 - val_loss: 0.7377\n",
            "Epoch 77/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6980 - val_loss: 0.7362\n",
            "Epoch 78/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6981 - val_loss: 0.7359\n",
            "Epoch 79/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 0.7387\n",
            "Epoch 80/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6976 - val_loss: 0.7348\n",
            "Epoch 81/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6966 - val_loss: 0.7384\n",
            "Epoch 82/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - val_loss: 0.7378\n",
            "Epoch 83/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6968 - val_loss: 0.7335\n",
            "Epoch 84/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6973 - val_loss: 0.7367\n",
            "Epoch 85/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 0.7351\n",
            "Epoch 86/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 0.7363\n",
            "Epoch 87/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6964 - val_loss: 0.7339\n",
            "Epoch 88/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6959 - val_loss: 0.7335\n",
            "Epoch 89/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6956 - val_loss: 0.7381\n",
            "Epoch 90/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6965 - val_loss: 0.7357\n",
            "Epoch 91/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6954 - val_loss: 0.7331\n",
            "Epoch 92/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6948 - val_loss: 0.7336\n",
            "Epoch 93/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6943 - val_loss: 0.7313\n",
            "Epoch 94/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6938 - val_loss: 0.7315\n",
            "Epoch 95/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6941 - val_loss: 0.7339\n",
            "Epoch 96/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6945 - val_loss: 0.7331\n",
            "Epoch 97/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6923 - val_loss: 0.7315\n",
            "Epoch 98/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6942 - val_loss: 0.7317\n",
            "Epoch 99/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6921 - val_loss: 0.7358\n",
            "Epoch 100/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6925 - val_loss: 0.7306\n",
            "Epoch 101/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6916 - val_loss: 0.7304\n",
            "Epoch 102/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6913 - val_loss: 0.7314\n",
            "Epoch 103/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6906 - val_loss: 0.7318\n",
            "Epoch 104/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6907 - val_loss: 0.7280\n",
            "Epoch 105/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6895 - val_loss: 0.7285\n",
            "Epoch 106/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6892 - val_loss: 0.7249\n",
            "Epoch 107/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6885 - val_loss: 0.7293\n",
            "Epoch 108/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6881 - val_loss: 0.7266\n",
            "Epoch 109/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6876 - val_loss: 0.7255\n",
            "Epoch 110/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6865 - val_loss: 0.7267\n",
            "Epoch 111/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6857 - val_loss: 0.7221\n",
            "Epoch 112/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6852 - val_loss: 0.7235\n",
            "Epoch 113/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6842 - val_loss: 0.7245\n",
            "Epoch 114/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6834 - val_loss: 0.7259\n",
            "Epoch 115/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6832 - val_loss: 0.7208\n",
            "Epoch 116/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6822 - val_loss: 0.7175\n",
            "Epoch 117/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6810 - val_loss: 0.7186\n",
            "Epoch 118/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6797 - val_loss: 0.7187\n",
            "Epoch 119/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6791 - val_loss: 0.7149\n",
            "Epoch 120/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6772 - val_loss: 0.7149\n",
            "Epoch 121/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6761 - val_loss: 0.7136\n",
            "Epoch 122/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6745 - val_loss: 0.7119\n",
            "Epoch 123/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6733 - val_loss: 0.7111\n",
            "Epoch 124/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6717 - val_loss: 0.7083\n",
            "Epoch 125/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6701 - val_loss: 0.7070\n",
            "Epoch 126/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6678 - val_loss: 0.7061\n",
            "Epoch 127/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6663 - val_loss: 0.7065\n",
            "Epoch 128/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6643 - val_loss: 0.7001\n",
            "Epoch 129/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6616 - val_loss: 0.7028\n",
            "Epoch 130/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6599 - val_loss: 0.6978\n",
            "Epoch 131/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6574 - val_loss: 0.6960\n",
            "Epoch 132/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6540 - val_loss: 0.6895\n",
            "Epoch 133/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6510 - val_loss: 0.6857\n",
            "Epoch 134/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6481 - val_loss: 0.6831\n",
            "Epoch 135/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6446 - val_loss: 0.6785\n",
            "Epoch 136/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6405 - val_loss: 0.6759\n",
            "Epoch 137/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6369 - val_loss: 0.6718\n",
            "Epoch 138/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6315 - val_loss: 0.6667\n",
            "Epoch 139/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6274 - val_loss: 0.6655\n",
            "Epoch 140/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6225 - val_loss: 0.6566\n",
            "Epoch 141/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6168 - val_loss: 0.6503\n",
            "Epoch 142/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6105 - val_loss: 0.6473\n",
            "Epoch 143/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6033 - val_loss: 0.6392\n",
            "Epoch 144/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5983 - val_loss: 0.6324\n",
            "Epoch 145/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5914 - val_loss: 0.6253\n",
            "Epoch 146/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5831 - val_loss: 0.6176\n",
            "Epoch 147/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5761 - val_loss: 0.6123\n",
            "Epoch 148/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5695 - val_loss: 0.6034\n",
            "Epoch 149/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5614 - val_loss: 0.5951\n",
            "Epoch 150/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5528 - val_loss: 0.5861\n",
            "Model with sigmoid activation, 225 epoch and 0.1 learning rate\n",
            "Epoch 1/225\n",
            "32/32 [==============================] - 1s 14ms/step - loss: 1.2793 - val_loss: 0.7444\n",
            "Epoch 2/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7041 - val_loss: 0.7409\n",
            "Epoch 3/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7047 - val_loss: 0.7420\n",
            "Epoch 4/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7034 - val_loss: 0.7473\n",
            "Epoch 5/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7045 - val_loss: 0.7421\n",
            "Epoch 6/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7039 - val_loss: 0.7442\n",
            "Epoch 7/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7045 - val_loss: 0.7427\n",
            "Epoch 8/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7040 - val_loss: 0.7426\n",
            "Epoch 9/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7038 - val_loss: 0.7437\n",
            "Epoch 10/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7038 - val_loss: 0.7429\n",
            "Epoch 11/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7037 - val_loss: 0.7421\n",
            "Epoch 12/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7413\n",
            "Epoch 13/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7035 - val_loss: 0.7418\n",
            "Epoch 14/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7042 - val_loss: 0.7412\n",
            "Epoch 15/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7030 - val_loss: 0.7460\n",
            "Epoch 16/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.7440\n",
            "Epoch 17/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 0.7443\n",
            "Epoch 18/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7035 - val_loss: 0.7427\n",
            "Epoch 19/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7025 - val_loss: 0.7401\n",
            "Epoch 20/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7030 - val_loss: 0.7395\n",
            "Epoch 21/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7024 - val_loss: 0.7401\n",
            "Epoch 22/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7422\n",
            "Epoch 23/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7021 - val_loss: 0.7397\n",
            "Epoch 24/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7414\n",
            "Epoch 25/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7018 - val_loss: 0.7422\n",
            "Epoch 26/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.7427\n",
            "Epoch 27/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7014 - val_loss: 0.7428\n",
            "Epoch 28/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.7398\n",
            "Epoch 29/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7433\n",
            "Epoch 30/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.7422\n",
            "Epoch 31/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7008 - val_loss: 0.7430\n",
            "Epoch 32/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7404\n",
            "Epoch 33/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.7387\n",
            "Epoch 34/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7011 - val_loss: 0.7400\n",
            "Epoch 35/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7399\n",
            "Epoch 36/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7007 - val_loss: 0.7412\n",
            "Epoch 37/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7004 - val_loss: 0.7386\n",
            "Epoch 38/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7003 - val_loss: 0.7370\n",
            "Epoch 39/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7423\n",
            "Epoch 40/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6996 - val_loss: 0.7391\n",
            "Epoch 41/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.7386\n",
            "Epoch 42/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7373\n",
            "Epoch 43/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.7376\n",
            "Epoch 44/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6987 - val_loss: 0.7401\n",
            "Epoch 45/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6992 - val_loss: 0.7404\n",
            "Epoch 46/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6989 - val_loss: 0.7411\n",
            "Epoch 47/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6990 - val_loss: 0.7365\n",
            "Epoch 48/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6979 - val_loss: 0.7372\n",
            "Epoch 49/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.7383\n",
            "Epoch 50/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6974 - val_loss: 0.7351\n",
            "Epoch 51/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6973 - val_loss: 0.7343\n",
            "Epoch 52/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6969 - val_loss: 0.7353\n",
            "Epoch 53/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6968 - val_loss: 0.7369\n",
            "Epoch 54/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6961 - val_loss: 0.7355\n",
            "Epoch 55/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6961 - val_loss: 0.7343\n",
            "Epoch 56/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6956 - val_loss: 0.7368\n",
            "Epoch 57/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6950 - val_loss: 0.7325\n",
            "Epoch 58/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 0.7372\n",
            "Epoch 59/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6940 - val_loss: 0.7347\n",
            "Epoch 60/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6939 - val_loss: 0.7340\n",
            "Epoch 61/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6932 - val_loss: 0.7319\n",
            "Epoch 62/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6922 - val_loss: 0.7316\n",
            "Epoch 63/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6916 - val_loss: 0.7314\n",
            "Epoch 64/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6907 - val_loss: 0.7302\n",
            "Epoch 65/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6906 - val_loss: 0.7301\n",
            "Epoch 66/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6895 - val_loss: 0.7287\n",
            "Epoch 67/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6886 - val_loss: 0.7281\n",
            "Epoch 68/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6879 - val_loss: 0.7284\n",
            "Epoch 69/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6873 - val_loss: 0.7269\n",
            "Epoch 70/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6856 - val_loss: 0.7265\n",
            "Epoch 71/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6849 - val_loss: 0.7238\n",
            "Epoch 72/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6838 - val_loss: 0.7232\n",
            "Epoch 73/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 0.7240\n",
            "Epoch 74/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6818 - val_loss: 0.7212\n",
            "Epoch 75/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6804 - val_loss: 0.7197\n",
            "Epoch 76/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6792 - val_loss: 0.7170\n",
            "Epoch 77/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6771 - val_loss: 0.7148\n",
            "Epoch 78/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6750 - val_loss: 0.7153\n",
            "Epoch 79/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6734 - val_loss: 0.7122\n",
            "Epoch 80/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6724 - val_loss: 0.7101\n",
            "Epoch 81/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6687 - val_loss: 0.7100\n",
            "Epoch 82/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6676 - val_loss: 0.7046\n",
            "Epoch 83/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6644 - val_loss: 0.7033\n",
            "Epoch 84/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6611 - val_loss: 0.7027\n",
            "Epoch 85/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6590 - val_loss: 0.6960\n",
            "Epoch 86/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6554 - val_loss: 0.6955\n",
            "Epoch 87/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6520 - val_loss: 0.6907\n",
            "Epoch 88/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6479 - val_loss: 0.6872\n",
            "Epoch 89/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6441 - val_loss: 0.6830\n",
            "Epoch 90/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6403 - val_loss: 0.6785\n",
            "Epoch 91/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6349 - val_loss: 0.6769\n",
            "Epoch 92/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6308 - val_loss: 0.6682\n",
            "Epoch 93/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6254 - val_loss: 0.6653\n",
            "Epoch 94/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6198 - val_loss: 0.6589\n",
            "Epoch 95/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6137 - val_loss: 0.6502\n",
            "Epoch 96/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6082 - val_loss: 0.6450\n",
            "Epoch 97/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6024 - val_loss: 0.6385\n",
            "Epoch 98/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5958 - val_loss: 0.6327\n",
            "Epoch 99/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5901 - val_loss: 0.6270\n",
            "Epoch 100/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5836 - val_loss: 0.6205\n",
            "Epoch 101/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5773 - val_loss: 0.6137\n",
            "Epoch 102/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5718 - val_loss: 0.6081\n",
            "Epoch 103/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5660 - val_loss: 0.6015\n",
            "Epoch 104/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5603 - val_loss: 0.5969\n",
            "Epoch 105/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5553 - val_loss: 0.5909\n",
            "Epoch 106/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5511 - val_loss: 0.5866\n",
            "Epoch 107/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5458 - val_loss: 0.5816\n",
            "Epoch 108/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5421 - val_loss: 0.5793\n",
            "Epoch 109/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5382 - val_loss: 0.5745\n",
            "Epoch 110/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5348 - val_loss: 0.5709\n",
            "Epoch 111/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5317 - val_loss: 0.5676\n",
            "Epoch 112/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5291 - val_loss: 0.5629\n",
            "Epoch 113/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5261 - val_loss: 0.5587\n",
            "Epoch 114/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5239 - val_loss: 0.5596\n",
            "Epoch 115/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5218 - val_loss: 0.5552\n",
            "Epoch 116/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5195 - val_loss: 0.5526\n",
            "Epoch 117/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5177 - val_loss: 0.5493\n",
            "Epoch 118/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5160 - val_loss: 0.5477\n",
            "Epoch 119/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5145 - val_loss: 0.5465\n",
            "Epoch 120/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5131 - val_loss: 0.5463\n",
            "Epoch 121/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5118 - val_loss: 0.5451\n",
            "Epoch 122/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5104 - val_loss: 0.5432\n",
            "Epoch 123/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5088 - val_loss: 0.5443\n",
            "Epoch 124/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5083 - val_loss: 0.5421\n",
            "Epoch 125/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5074 - val_loss: 0.5401\n",
            "Epoch 126/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5059 - val_loss: 0.5378\n",
            "Epoch 127/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5050 - val_loss: 0.5373\n",
            "Epoch 128/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5043 - val_loss: 0.5377\n",
            "Epoch 129/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5034 - val_loss: 0.5341\n",
            "Epoch 130/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5022 - val_loss: 0.5353\n",
            "Epoch 131/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5020 - val_loss: 0.5366\n",
            "Epoch 132/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5009 - val_loss: 0.5315\n",
            "Epoch 133/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5003 - val_loss: 0.5308\n",
            "Epoch 134/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4996 - val_loss: 0.5315\n",
            "Epoch 135/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4990 - val_loss: 0.5292\n",
            "Epoch 136/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4976 - val_loss: 0.5314\n",
            "Epoch 137/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4977 - val_loss: 0.5281\n",
            "Epoch 138/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4973 - val_loss: 0.5295\n",
            "Epoch 139/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4965 - val_loss: 0.5283\n",
            "Epoch 140/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4959 - val_loss: 0.5257\n",
            "Epoch 141/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4951 - val_loss: 0.5236\n",
            "Epoch 142/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4948 - val_loss: 0.5231\n",
            "Epoch 143/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4942 - val_loss: 0.5234\n",
            "Epoch 144/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4937 - val_loss: 0.5238\n",
            "Epoch 145/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4932 - val_loss: 0.5226\n",
            "Epoch 146/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4926 - val_loss: 0.5229\n",
            "Epoch 147/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4922 - val_loss: 0.5235\n",
            "Epoch 148/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4917 - val_loss: 0.5191\n",
            "Epoch 149/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4911 - val_loss: 0.5189\n",
            "Epoch 150/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4904 - val_loss: 0.5191\n",
            "Epoch 151/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4899 - val_loss: 0.5180\n",
            "Epoch 152/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4897 - val_loss: 0.5176\n",
            "Epoch 153/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4889 - val_loss: 0.5186\n",
            "Epoch 154/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4888 - val_loss: 0.5206\n",
            "Epoch 155/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4880 - val_loss: 0.5172\n",
            "Epoch 156/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4874 - val_loss: 0.5148\n",
            "Epoch 157/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4869 - val_loss: 0.5139\n",
            "Epoch 158/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4865 - val_loss: 0.5135\n",
            "Epoch 159/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4858 - val_loss: 0.5152\n",
            "Epoch 160/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4855 - val_loss: 0.5136\n",
            "Epoch 161/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4847 - val_loss: 0.5123\n",
            "Epoch 162/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4838 - val_loss: 0.5118\n",
            "Epoch 163/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4838 - val_loss: 0.5119\n",
            "Epoch 164/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4827 - val_loss: 0.5120\n",
            "Epoch 165/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4827 - val_loss: 0.5085\n",
            "Epoch 166/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4816 - val_loss: 0.5087\n",
            "Epoch 167/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4813 - val_loss: 0.5071\n",
            "Epoch 168/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4801 - val_loss: 0.5062\n",
            "Epoch 169/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4797 - val_loss: 0.5070\n",
            "Epoch 170/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4785 - val_loss: 0.5065\n",
            "Epoch 171/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4777 - val_loss: 0.5042\n",
            "Epoch 172/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4769 - val_loss: 0.5037\n",
            "Epoch 173/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4764 - val_loss: 0.5035\n",
            "Epoch 174/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4749 - val_loss: 0.5007\n",
            "Epoch 175/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4745 - val_loss: 0.4996\n",
            "Epoch 176/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4731 - val_loss: 0.4984\n",
            "Epoch 177/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4721 - val_loss: 0.4967\n",
            "Epoch 178/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4709 - val_loss: 0.4950\n",
            "Epoch 179/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4695 - val_loss: 0.4942\n",
            "Epoch 180/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4682 - val_loss: 0.4922\n",
            "Epoch 181/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4669 - val_loss: 0.4919\n",
            "Epoch 182/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4654 - val_loss: 0.4907\n",
            "Epoch 183/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4639 - val_loss: 0.4880\n",
            "Epoch 184/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4619 - val_loss: 0.4851\n",
            "Epoch 185/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4605 - val_loss: 0.4854\n",
            "Epoch 186/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4585 - val_loss: 0.4812\n",
            "Epoch 187/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4564 - val_loss: 0.4789\n",
            "Epoch 188/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4537 - val_loss: 0.4779\n",
            "Epoch 189/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4519 - val_loss: 0.4770\n",
            "Epoch 190/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4495 - val_loss: 0.4725\n",
            "Epoch 191/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4470 - val_loss: 0.4705\n",
            "Epoch 192/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4439 - val_loss: 0.4680\n",
            "Epoch 193/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4404 - val_loss: 0.4611\n",
            "Epoch 194/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4377 - val_loss: 0.4585\n",
            "Epoch 195/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4345 - val_loss: 0.4559\n",
            "Epoch 196/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4309 - val_loss: 0.4500\n",
            "Epoch 197/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4275 - val_loss: 0.4463\n",
            "Epoch 198/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4235 - val_loss: 0.4429\n",
            "Epoch 199/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4197 - val_loss: 0.4395\n",
            "Epoch 200/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4155 - val_loss: 0.4369\n",
            "Epoch 201/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4118 - val_loss: 0.4302\n",
            "Epoch 202/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4071 - val_loss: 0.4252\n",
            "Epoch 203/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4028 - val_loss: 0.4197\n",
            "Epoch 204/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3984 - val_loss: 0.4167\n",
            "Epoch 205/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3944 - val_loss: 0.4120\n",
            "Epoch 206/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3905 - val_loss: 0.4060\n",
            "Epoch 207/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3862 - val_loss: 0.4029\n",
            "Epoch 208/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3825 - val_loss: 0.4034\n",
            "Epoch 209/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3789 - val_loss: 0.3943\n",
            "Epoch 210/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3753 - val_loss: 0.3902\n",
            "Epoch 211/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3722 - val_loss: 0.3874\n",
            "Epoch 212/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3691 - val_loss: 0.3872\n",
            "Epoch 213/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3665 - val_loss: 0.3813\n",
            "Epoch 214/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3638 - val_loss: 0.3800\n",
            "Epoch 215/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3611 - val_loss: 0.3779\n",
            "Epoch 216/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3599 - val_loss: 0.3757\n",
            "Epoch 217/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3579 - val_loss: 0.3719\n",
            "Epoch 218/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3560 - val_loss: 0.3715\n",
            "Epoch 219/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3544 - val_loss: 0.3690\n",
            "Epoch 220/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3531 - val_loss: 0.3684\n",
            "Epoch 221/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3517 - val_loss: 0.3666\n",
            "Epoch 222/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3506 - val_loss: 0.3670\n",
            "Epoch 223/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3496 - val_loss: 0.3649\n",
            "Epoch 224/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3483 - val_loss: 0.3652\n",
            "Epoch 225/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3482 - val_loss: 0.3628\n",
            "Model with sigmoid activation, 100 epoch and 0.2 learning rate\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 13ms/step - loss: 1.0488 - val_loss: 0.7493\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7077 - val_loss: 0.7482\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7084 - val_loss: 0.7449\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7077 - val_loss: 0.7635\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7079 - val_loss: 0.7458\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7076 - val_loss: 0.7453\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7071 - val_loss: 0.7484\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7066 - val_loss: 0.7487\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7071 - val_loss: 0.7427\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7064 - val_loss: 0.7416\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7064 - val_loss: 0.7436\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7058 - val_loss: 0.7458\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7063 - val_loss: 0.7571\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 0.7496\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7063 - val_loss: 0.7447\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7062 - val_loss: 0.7494\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7066 - val_loss: 0.7460\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7057 - val_loss: 0.7414\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7052 - val_loss: 0.7446\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7057 - val_loss: 0.7486\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7053 - val_loss: 0.7491\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7049 - val_loss: 0.7480\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7049 - val_loss: 0.7462\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7059 - val_loss: 0.7417\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7053 - val_loss: 0.7413\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7037 - val_loss: 0.7442\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7027 - val_loss: 0.7471\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7049 - val_loss: 0.7432\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - val_loss: 0.7391\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7390\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7031 - val_loss: 0.7426\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7043 - val_loss: 0.7441\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7029 - val_loss: 0.7447\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7459\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.7460\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7028 - val_loss: 0.7387\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7015 - val_loss: 0.7432\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7023 - val_loss: 0.7457\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.7396\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7015 - val_loss: 0.7379\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7013 - val_loss: 0.7402\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6997 - val_loss: 0.7501\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.7365\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7369\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7333\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6987 - val_loss: 0.7421\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6990 - val_loss: 0.7381\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6985 - val_loss: 0.7353\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6982 - val_loss: 0.7358\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6971 - val_loss: 0.7325\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6949 - val_loss: 0.7489\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6966 - val_loss: 0.7333\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6947 - val_loss: 0.7375\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6952 - val_loss: 0.7305\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6929 - val_loss: 0.7318\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6925 - val_loss: 0.7256\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6907 - val_loss: 0.7289\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6909 - val_loss: 0.7237\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6876 - val_loss: 0.7266\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6870 - val_loss: 0.7230\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6846 - val_loss: 0.7236\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6825 - val_loss: 0.7161\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6804 - val_loss: 0.7264\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6782 - val_loss: 0.7173\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6753 - val_loss: 0.7124\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6721 - val_loss: 0.7054\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6673 - val_loss: 0.7036\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6642 - val_loss: 0.6991\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6582 - val_loss: 0.6970\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6514 - val_loss: 0.6936\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6448 - val_loss: 0.6782\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6371 - val_loss: 0.6683\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6291 - val_loss: 0.6564\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6173 - val_loss: 0.6454\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6061 - val_loss: 0.6352\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5955 - val_loss: 0.6205\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5830 - val_loss: 0.6207\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5728 - val_loss: 0.5970\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5586 - val_loss: 0.5819\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5504 - val_loss: 0.5774\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5439 - val_loss: 0.5742\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5362 - val_loss: 0.5591\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5299 - val_loss: 0.5673\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5262 - val_loss: 0.5495\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5214 - val_loss: 0.5449\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5176 - val_loss: 0.5422\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5141 - val_loss: 0.5423\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5115 - val_loss: 0.5370\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5079 - val_loss: 0.5316\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5049 - val_loss: 0.5305\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5018 - val_loss: 0.5326\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5011 - val_loss: 0.5218\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.5188\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4940 - val_loss: 0.5178\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4912 - val_loss: 0.5136\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4882 - val_loss: 0.5091\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4848 - val_loss: 0.5078\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4811 - val_loss: 0.5043\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4778 - val_loss: 0.4991\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4721 - val_loss: 0.4920\n",
            "Model with sigmoid activation, 150 epoch and 0.2 learning rate\n",
            "Epoch 1/150\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 1.1508 - val_loss: 0.7393\n",
            "Epoch 2/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7036 - val_loss: 0.7469\n",
            "Epoch 3/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7406\n",
            "Epoch 4/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7019 - val_loss: 0.7523\n",
            "Epoch 5/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7025 - val_loss: 0.7477\n",
            "Epoch 6/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7011 - val_loss: 0.7411\n",
            "Epoch 7/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7006 - val_loss: 0.7416\n",
            "Epoch 8/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7472\n",
            "Epoch 9/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7010 - val_loss: 0.7391\n",
            "Epoch 10/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7000 - val_loss: 0.7393\n",
            "Epoch 11/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6998 - val_loss: 0.7406\n",
            "Epoch 12/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7397\n",
            "Epoch 13/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6994 - val_loss: 0.7369\n",
            "Epoch 14/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6982 - val_loss: 0.7346\n",
            "Epoch 15/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6983 - val_loss: 0.7379\n",
            "Epoch 16/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6971 - val_loss: 0.7372\n",
            "Epoch 17/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6958 - val_loss: 0.7356\n",
            "Epoch 18/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6952 - val_loss: 0.7452\n",
            "Epoch 19/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6954 - val_loss: 0.7340\n",
            "Epoch 20/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6943 - val_loss: 0.7544\n",
            "Epoch 21/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6952 - val_loss: 0.7402\n",
            "Epoch 22/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - val_loss: 0.7274\n",
            "Epoch 23/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6902 - val_loss: 0.7350\n",
            "Epoch 24/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6890 - val_loss: 0.7266\n",
            "Epoch 25/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6886 - val_loss: 0.7300\n",
            "Epoch 26/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6852 - val_loss: 0.7345\n",
            "Epoch 27/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6826 - val_loss: 0.7198\n",
            "Epoch 28/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6817 - val_loss: 0.7287\n",
            "Epoch 29/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6806 - val_loss: 0.7186\n",
            "Epoch 30/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6750 - val_loss: 0.7198\n",
            "Epoch 31/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6730 - val_loss: 0.7183\n",
            "Epoch 32/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6697 - val_loss: 0.7051\n",
            "Epoch 33/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6649 - val_loss: 0.7092\n",
            "Epoch 34/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6582 - val_loss: 0.6966\n",
            "Epoch 35/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6531 - val_loss: 0.6855\n",
            "Epoch 36/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6447 - val_loss: 0.6845\n",
            "Epoch 37/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6364 - val_loss: 0.6758\n",
            "Epoch 38/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6290 - val_loss: 0.6582\n",
            "Epoch 39/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6159 - val_loss: 0.6462\n",
            "Epoch 40/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6022 - val_loss: 0.6355\n",
            "Epoch 41/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5876 - val_loss: 0.6219\n",
            "Epoch 42/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5725 - val_loss: 0.6030\n",
            "Epoch 43/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5554 - val_loss: 0.5848\n",
            "Epoch 44/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5397 - val_loss: 0.5696\n",
            "Epoch 45/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5248 - val_loss: 0.5532\n",
            "Epoch 46/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5095 - val_loss: 0.5384\n",
            "Epoch 47/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4964 - val_loss: 0.5254\n",
            "Epoch 48/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4840 - val_loss: 0.5131\n",
            "Epoch 49/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4723 - val_loss: 0.4990\n",
            "Epoch 50/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4615 - val_loss: 0.4900\n",
            "Epoch 51/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4498 - val_loss: 0.4815\n",
            "Epoch 52/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4390 - val_loss: 0.4674\n",
            "Epoch 53/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4276 - val_loss: 0.4532\n",
            "Epoch 54/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4175 - val_loss: 0.4481\n",
            "Epoch 55/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4087 - val_loss: 0.4360\n",
            "Epoch 56/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4018 - val_loss: 0.4298\n",
            "Epoch 57/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3950 - val_loss: 0.4212\n",
            "Epoch 58/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3901 - val_loss: 0.4157\n",
            "Epoch 59/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3856 - val_loss: 0.4096\n",
            "Epoch 60/150\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3815 - val_loss: 0.4063\n",
            "Epoch 61/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3794 - val_loss: 0.4026\n",
            "Epoch 62/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3772 - val_loss: 0.4023\n",
            "Epoch 63/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3756 - val_loss: 0.4009\n",
            "Epoch 64/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3742 - val_loss: 0.3977\n",
            "Epoch 65/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3730 - val_loss: 0.3961\n",
            "Epoch 66/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3720 - val_loss: 0.3962\n",
            "Epoch 67/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3711 - val_loss: 0.3964\n",
            "Epoch 68/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3705 - val_loss: 0.3946\n",
            "Epoch 69/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3687 - val_loss: 0.3947\n",
            "Epoch 70/150\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3687 - val_loss: 0.3963\n",
            "Epoch 71/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3675 - val_loss: 0.3887\n",
            "Epoch 72/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3669 - val_loss: 0.3912\n",
            "Epoch 73/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3678 - val_loss: 0.3879\n",
            "Epoch 74/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3663 - val_loss: 0.3896\n",
            "Epoch 75/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3662 - val_loss: 0.3887\n",
            "Epoch 76/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3657 - val_loss: 0.3847\n",
            "Epoch 77/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3651 - val_loss: 0.3877\n",
            "Epoch 78/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3642 - val_loss: 0.3886\n",
            "Epoch 79/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3641 - val_loss: 0.3892\n",
            "Epoch 80/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3635 - val_loss: 0.3877\n",
            "Epoch 81/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3631 - val_loss: 0.3854\n",
            "Epoch 82/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3624 - val_loss: 0.3833\n",
            "Epoch 83/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3627 - val_loss: 0.3871\n",
            "Epoch 84/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3627 - val_loss: 0.3815\n",
            "Epoch 85/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3618 - val_loss: 0.3838\n",
            "Epoch 86/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3618 - val_loss: 0.3855\n",
            "Epoch 87/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3610 - val_loss: 0.3807\n",
            "Epoch 88/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3604 - val_loss: 0.3813\n",
            "Epoch 89/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3608 - val_loss: 0.3826\n",
            "Epoch 90/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 0.3858\n",
            "Epoch 91/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3600 - val_loss: 0.3774\n",
            "Epoch 92/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3597 - val_loss: 0.3790\n",
            "Epoch 93/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3599 - val_loss: 0.3836\n",
            "Epoch 94/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3591 - val_loss: 0.3762\n",
            "Epoch 95/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3589 - val_loss: 0.3768\n",
            "Epoch 96/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3583 - val_loss: 0.3752\n",
            "Epoch 97/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3590 - val_loss: 0.3739\n",
            "Epoch 98/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3581 - val_loss: 0.3739\n",
            "Epoch 99/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3579 - val_loss: 0.3845\n",
            "Epoch 100/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3572 - val_loss: 0.3760\n",
            "Epoch 101/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3576 - val_loss: 0.3773\n",
            "Epoch 102/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3574 - val_loss: 0.3870\n",
            "Epoch 103/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3572 - val_loss: 0.3745\n",
            "Epoch 104/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3567 - val_loss: 0.3725\n",
            "Epoch 105/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3565 - val_loss: 0.3735\n",
            "Epoch 106/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3554 - val_loss: 0.3752\n",
            "Epoch 107/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3568 - val_loss: 0.3734\n",
            "Epoch 108/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3556 - val_loss: 0.3761\n",
            "Epoch 109/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3546 - val_loss: 0.3696\n",
            "Epoch 110/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3552 - val_loss: 0.3721\n",
            "Epoch 111/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3550 - val_loss: 0.3729\n",
            "Epoch 112/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3547 - val_loss: 0.3717\n",
            "Epoch 113/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3548 - val_loss: 0.3685\n",
            "Epoch 114/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3548 - val_loss: 0.3669\n",
            "Epoch 115/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3546 - val_loss: 0.3719\n",
            "Epoch 116/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3543 - val_loss: 0.3729\n",
            "Epoch 117/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3547 - val_loss: 0.3729\n",
            "Epoch 118/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3538 - val_loss: 0.3734\n",
            "Epoch 119/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3540 - val_loss: 0.3677\n",
            "Epoch 120/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3528 - val_loss: 0.3675\n",
            "Epoch 121/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3534 - val_loss: 0.3681\n",
            "Epoch 122/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3527 - val_loss: 0.3675\n",
            "Epoch 123/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3533 - val_loss: 0.3726\n",
            "Epoch 124/150\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3531 - val_loss: 0.3714\n",
            "Epoch 125/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3533 - val_loss: 0.3711\n",
            "Epoch 126/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3531 - val_loss: 0.3704\n",
            "Epoch 127/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3528 - val_loss: 0.3639\n",
            "Epoch 128/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3520 - val_loss: 0.3687\n",
            "Epoch 129/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3528 - val_loss: 0.3663\n",
            "Epoch 130/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3511 - val_loss: 0.3662\n",
            "Epoch 131/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 0.3631\n",
            "Epoch 132/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3518 - val_loss: 0.3666\n",
            "Epoch 133/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3518 - val_loss: 0.3671\n",
            "Epoch 134/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3515 - val_loss: 0.3677\n",
            "Epoch 135/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3513 - val_loss: 0.3652\n",
            "Epoch 136/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3518 - val_loss: 0.3617\n",
            "Epoch 137/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3511 - val_loss: 0.3616\n",
            "Epoch 138/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3508 - val_loss: 0.3782\n",
            "Epoch 139/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3515 - val_loss: 0.3609\n",
            "Epoch 140/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3503 - val_loss: 0.3591\n",
            "Epoch 141/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3496 - val_loss: 0.3706\n",
            "Epoch 142/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3510 - val_loss: 0.3610\n",
            "Epoch 143/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3502 - val_loss: 0.3648\n",
            "Epoch 144/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3502 - val_loss: 0.3585\n",
            "Epoch 145/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3491 - val_loss: 0.3610\n",
            "Epoch 146/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3501 - val_loss: 0.3609\n",
            "Epoch 147/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3497 - val_loss: 0.3598\n",
            "Epoch 148/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3488 - val_loss: 0.3575\n",
            "Epoch 149/150\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3499 - val_loss: 0.3649\n",
            "Epoch 150/150\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3494 - val_loss: 0.3639\n",
            "Model with sigmoid activation, 225 epoch and 0.2 learning rate\n",
            "Epoch 1/225\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.9633 - val_loss: 0.7455\n",
            "Epoch 2/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7086 - val_loss: 0.7450\n",
            "Epoch 3/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7081 - val_loss: 0.7501\n",
            "Epoch 4/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7074 - val_loss: 0.7488\n",
            "Epoch 5/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7072 - val_loss: 0.7472\n",
            "Epoch 6/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7053 - val_loss: 0.7447\n",
            "Epoch 7/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7071 - val_loss: 0.7452\n",
            "Epoch 8/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7059 - val_loss: 0.7477\n",
            "Epoch 9/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7067 - val_loss: 0.7505\n",
            "Epoch 10/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7043 - val_loss: 0.7475\n",
            "Epoch 11/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7055 - val_loss: 0.7410\n",
            "Epoch 12/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7047 - val_loss: 0.7433\n",
            "Epoch 13/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7063 - val_loss: 0.7446\n",
            "Epoch 14/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7054 - val_loss: 0.7563\n",
            "Epoch 15/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - val_loss: 0.7514\n",
            "Epoch 16/225\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7047 - val_loss: 0.7489\n",
            "Epoch 17/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7030 - val_loss: 0.7533\n",
            "Epoch 18/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7029 - val_loss: 0.7424\n",
            "Epoch 19/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7028 - val_loss: 0.7415\n",
            "Epoch 20/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.7393\n",
            "Epoch 21/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7004 - val_loss: 0.7504\n",
            "Epoch 22/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6998 - val_loss: 0.7381\n",
            "Epoch 23/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7007 - val_loss: 0.7379\n",
            "Epoch 24/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7001 - val_loss: 0.7438\n",
            "Epoch 25/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6979 - val_loss: 0.7367\n",
            "Epoch 26/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6976 - val_loss: 0.7396\n",
            "Epoch 27/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6973 - val_loss: 0.7344\n",
            "Epoch 28/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6955 - val_loss: 0.7333\n",
            "Epoch 29/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6936 - val_loss: 0.7291\n",
            "Epoch 30/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6902 - val_loss: 0.7372\n",
            "Epoch 31/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6912 - val_loss: 0.7304\n",
            "Epoch 32/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6896 - val_loss: 0.7310\n",
            "Epoch 33/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6880 - val_loss: 0.7236\n",
            "Epoch 34/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6830 - val_loss: 0.7227\n",
            "Epoch 35/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6813 - val_loss: 0.7215\n",
            "Epoch 36/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6769 - val_loss: 0.7108\n",
            "Epoch 37/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6731 - val_loss: 0.7141\n",
            "Epoch 38/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6680 - val_loss: 0.7150\n",
            "Epoch 39/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6628 - val_loss: 0.6992\n",
            "Epoch 40/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6559 - val_loss: 0.6883\n",
            "Epoch 41/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6475 - val_loss: 0.6947\n",
            "Epoch 42/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6404 - val_loss: 0.6789\n",
            "Epoch 43/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6281 - val_loss: 0.6684\n",
            "Epoch 44/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6176 - val_loss: 0.6518\n",
            "Epoch 45/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6066 - val_loss: 0.6546\n",
            "Epoch 46/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.5939 - val_loss: 0.6283\n",
            "Epoch 47/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5805 - val_loss: 0.6150\n",
            "Epoch 48/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5678 - val_loss: 0.6023\n",
            "Epoch 49/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5575 - val_loss: 0.5878\n",
            "Epoch 50/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5476 - val_loss: 0.5857\n",
            "Epoch 51/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5387 - val_loss: 0.5797\n",
            "Epoch 52/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5329 - val_loss: 0.5650\n",
            "Epoch 53/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5281 - val_loss: 0.5637\n",
            "Epoch 54/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5227 - val_loss: 0.5638\n",
            "Epoch 55/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5219 - val_loss: 0.5595\n",
            "Epoch 56/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5192 - val_loss: 0.5535\n",
            "Epoch 57/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5176 - val_loss: 0.5488\n",
            "Epoch 58/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5147 - val_loss: 0.5520\n",
            "Epoch 59/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5136 - val_loss: 0.5480\n",
            "Epoch 60/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5122 - val_loss: 0.5596\n",
            "Epoch 61/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5119 - val_loss: 0.5545\n",
            "Epoch 62/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5122 - val_loss: 0.5508\n",
            "Epoch 63/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5106 - val_loss: 0.5419\n",
            "Epoch 64/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5082 - val_loss: 0.5403\n",
            "Epoch 65/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5067 - val_loss: 0.5455\n",
            "Epoch 66/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5059 - val_loss: 0.5443\n",
            "Epoch 67/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5051 - val_loss: 0.5393\n",
            "Epoch 68/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5039 - val_loss: 0.5425\n",
            "Epoch 69/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5048 - val_loss: 0.5421\n",
            "Epoch 70/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5040 - val_loss: 0.5355\n",
            "Epoch 71/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5033 - val_loss: 0.5332\n",
            "Epoch 72/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5009 - val_loss: 0.5320\n",
            "Epoch 73/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5002 - val_loss: 0.5351\n",
            "Epoch 74/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4998 - val_loss: 0.5317\n",
            "Epoch 75/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4985 - val_loss: 0.5330\n",
            "Epoch 76/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4984 - val_loss: 0.5299\n",
            "Epoch 77/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4963 - val_loss: 0.5410\n",
            "Epoch 78/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4964 - val_loss: 0.5340\n",
            "Epoch 79/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4939 - val_loss: 0.5320\n",
            "Epoch 80/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4931 - val_loss: 0.5224\n",
            "Epoch 81/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4916 - val_loss: 0.5223\n",
            "Epoch 82/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4905 - val_loss: 0.5230\n",
            "Epoch 83/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4869 - val_loss: 0.5170\n",
            "Epoch 84/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4850 - val_loss: 0.5134\n",
            "Epoch 85/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4839 - val_loss: 0.5160\n",
            "Epoch 86/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4817 - val_loss: 0.5095\n",
            "Epoch 87/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4783 - val_loss: 0.5074\n",
            "Epoch 88/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4753 - val_loss: 0.5034\n",
            "Epoch 89/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4704 - val_loss: 0.5069\n",
            "Epoch 90/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4677 - val_loss: 0.4926\n",
            "Epoch 91/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4626 - val_loss: 0.4881\n",
            "Epoch 92/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4559 - val_loss: 0.4805\n",
            "Epoch 93/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4510 - val_loss: 0.4818\n",
            "Epoch 94/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4433 - val_loss: 0.4638\n",
            "Epoch 95/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4335 - val_loss: 0.4561\n",
            "Epoch 96/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4261 - val_loss: 0.4459\n",
            "Epoch 97/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4149 - val_loss: 0.4370\n",
            "Epoch 98/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4062 - val_loss: 0.4291\n",
            "Epoch 99/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3960 - val_loss: 0.4185\n",
            "Epoch 100/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3856 - val_loss: 0.4059\n",
            "Epoch 101/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3771 - val_loss: 0.3941\n",
            "Epoch 102/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3692 - val_loss: 0.3872\n",
            "Epoch 103/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3624 - val_loss: 0.3894\n",
            "Epoch 104/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3575 - val_loss: 0.3785\n",
            "Epoch 105/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3537 - val_loss: 0.3787\n",
            "Epoch 106/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3501 - val_loss: 0.3741\n",
            "Epoch 107/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3479 - val_loss: 0.3757\n",
            "Epoch 108/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3471 - val_loss: 0.3693\n",
            "Epoch 109/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3451 - val_loss: 0.3708\n",
            "Epoch 110/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3449 - val_loss: 0.3647\n",
            "Epoch 111/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3449 - val_loss: 0.3642\n",
            "Epoch 112/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3440 - val_loss: 0.3640\n",
            "Epoch 113/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3440 - val_loss: 0.3640\n",
            "Epoch 114/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3437 - val_loss: 0.3637\n",
            "Epoch 115/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3427 - val_loss: 0.3643\n",
            "Epoch 116/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3433 - val_loss: 0.3649\n",
            "Epoch 117/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3423 - val_loss: 0.3626\n",
            "Epoch 118/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3420 - val_loss: 0.3652\n",
            "Epoch 119/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3415 - val_loss: 0.3626\n",
            "Epoch 120/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3416 - val_loss: 0.3620\n",
            "Epoch 121/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3420 - val_loss: 0.3640\n",
            "Epoch 122/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3413 - val_loss: 0.3641\n",
            "Epoch 123/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3405 - val_loss: 0.3585\n",
            "Epoch 124/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3407 - val_loss: 0.3616\n",
            "Epoch 125/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3411 - val_loss: 0.3599\n",
            "Epoch 126/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3402 - val_loss: 0.3594\n",
            "Epoch 127/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3400 - val_loss: 0.3600\n",
            "Epoch 128/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3406 - val_loss: 0.3589\n",
            "Epoch 129/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3396 - val_loss: 0.3601\n",
            "Epoch 130/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3403 - val_loss: 0.3575\n",
            "Epoch 131/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3395 - val_loss: 0.3600\n",
            "Epoch 132/225\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3393 - val_loss: 0.3578\n",
            "Epoch 133/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3398 - val_loss: 0.3597\n",
            "Epoch 134/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3393 - val_loss: 0.3622\n",
            "Epoch 135/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3393 - val_loss: 0.3583\n",
            "Epoch 136/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3393 - val_loss: 0.3626\n",
            "Epoch 137/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3390 - val_loss: 0.3586\n",
            "Epoch 138/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3386 - val_loss: 0.3579\n",
            "Epoch 139/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3395 - val_loss: 0.3573\n",
            "Epoch 140/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3389 - val_loss: 0.3629\n",
            "Epoch 141/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3395 - val_loss: 0.3595\n",
            "Epoch 142/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3384 - val_loss: 0.3560\n",
            "Epoch 143/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3388 - val_loss: 0.3602\n",
            "Epoch 144/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3389 - val_loss: 0.3577\n",
            "Epoch 145/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3388 - val_loss: 0.3576\n",
            "Epoch 146/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3387 - val_loss: 0.3582\n",
            "Epoch 147/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3388 - val_loss: 0.3583\n",
            "Epoch 148/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3378 - val_loss: 0.3589\n",
            "Epoch 149/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3380 - val_loss: 0.3593\n",
            "Epoch 150/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3384 - val_loss: 0.3603\n",
            "Epoch 151/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3381 - val_loss: 0.3562\n",
            "Epoch 152/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3378 - val_loss: 0.3548\n",
            "Epoch 153/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3381 - val_loss: 0.3569\n",
            "Epoch 154/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3375 - val_loss: 0.3583\n",
            "Epoch 155/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3377 - val_loss: 0.3552\n",
            "Epoch 156/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3382 - val_loss: 0.3550\n",
            "Epoch 157/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3378 - val_loss: 0.3584\n",
            "Epoch 158/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3376 - val_loss: 0.3543\n",
            "Epoch 159/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3365 - val_loss: 0.3532\n",
            "Epoch 160/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3372 - val_loss: 0.3570\n",
            "Epoch 161/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3377 - val_loss: 0.3559\n",
            "Epoch 162/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3372 - val_loss: 0.3587\n",
            "Epoch 163/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3380 - val_loss: 0.3544\n",
            "Epoch 164/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3367 - val_loss: 0.3545\n",
            "Epoch 165/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3369 - val_loss: 0.3573\n",
            "Epoch 166/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3367 - val_loss: 0.3573\n",
            "Epoch 167/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3371 - val_loss: 0.3578\n",
            "Epoch 168/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3368 - val_loss: 0.3549\n",
            "Epoch 169/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 0.3531\n",
            "Epoch 170/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3370 - val_loss: 0.3554\n",
            "Epoch 171/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3357 - val_loss: 0.3584\n",
            "Epoch 172/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3369 - val_loss: 0.3544\n",
            "Epoch 173/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3360 - val_loss: 0.3543\n",
            "Epoch 174/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3361 - val_loss: 0.3651\n",
            "Epoch 175/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3365 - val_loss: 0.3535\n",
            "Epoch 176/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3351 - val_loss: 0.3571\n",
            "Epoch 177/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3367 - val_loss: 0.3544\n",
            "Epoch 178/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3366 - val_loss: 0.3549\n",
            "Epoch 179/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3362 - val_loss: 0.3545\n",
            "Epoch 180/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3357 - val_loss: 0.3528\n",
            "Epoch 181/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3359 - val_loss: 0.3563\n",
            "Epoch 182/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3360 - val_loss: 0.3547\n",
            "Epoch 183/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3357 - val_loss: 0.3530\n",
            "Epoch 184/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3355 - val_loss: 0.3531\n",
            "Epoch 185/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3352 - val_loss: 0.3617\n",
            "Epoch 186/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3364 - val_loss: 0.3593\n",
            "Epoch 187/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3353 - val_loss: 0.3537\n",
            "Epoch 188/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3350 - val_loss: 0.3566\n",
            "Epoch 189/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3360 - val_loss: 0.3544\n",
            "Epoch 190/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3355 - val_loss: 0.3563\n",
            "Epoch 191/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3352 - val_loss: 0.3555\n",
            "Epoch 192/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3352 - val_loss: 0.3531\n",
            "Epoch 193/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3353 - val_loss: 0.3531\n",
            "Epoch 194/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3353 - val_loss: 0.3530\n",
            "Epoch 195/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3349 - val_loss: 0.3563\n",
            "Epoch 196/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3347 - val_loss: 0.3533\n",
            "Epoch 197/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3347 - val_loss: 0.3554\n",
            "Epoch 198/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3351 - val_loss: 0.3536\n",
            "Epoch 199/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3350 - val_loss: 0.3536\n",
            "Epoch 200/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3345 - val_loss: 0.3541\n",
            "Epoch 201/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3347 - val_loss: 0.3512\n",
            "Epoch 202/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3341 - val_loss: 0.3557\n",
            "Epoch 203/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3343 - val_loss: 0.3543\n",
            "Epoch 204/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3347 - val_loss: 0.3538\n",
            "Epoch 205/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3339 - val_loss: 0.3577\n",
            "Epoch 206/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3342 - val_loss: 0.3577\n",
            "Epoch 207/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3343 - val_loss: 0.3522\n",
            "Epoch 208/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3335 - val_loss: 0.3532\n",
            "Epoch 209/225\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3347 - val_loss: 0.3597\n",
            "Epoch 210/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3345 - val_loss: 0.3524\n",
            "Epoch 211/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3340 - val_loss: 0.3548\n",
            "Epoch 212/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3335 - val_loss: 0.3513\n",
            "Epoch 213/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3338 - val_loss: 0.3505\n",
            "Epoch 214/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3334 - val_loss: 0.3517\n",
            "Epoch 215/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3330 - val_loss: 0.3525\n",
            "Epoch 216/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3332 - val_loss: 0.3514\n",
            "Epoch 217/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 0.3523\n",
            "Epoch 218/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3328 - val_loss: 0.3521\n",
            "Epoch 219/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3323 - val_loss: 0.3533\n",
            "Epoch 220/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3329 - val_loss: 0.3525\n",
            "Epoch 221/225\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3324 - val_loss: 0.3510\n",
            "Epoch 222/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3325 - val_loss: 0.3508\n",
            "Epoch 223/225\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3327 - val_loss: 0.3523\n",
            "Epoch 224/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3325 - val_loss: 0.3518\n",
            "Epoch 225/225\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3315 - val_loss: 0.3561\n",
            "Model with relu activation, 100 epoch and 0.05 learning rate :   Train loss:0.48884883522987366   Test loss:0.511837363243103\n",
            "Model with relu activation, 150 epoch and 0.05 learning rate :   Train loss:0.18912722170352936   Test loss:0.20300935208797455\n",
            "Model with relu activation, 225 epoch and 0.05 learning rate :   Train loss:0.33096280694007874   Test loss:0.34861767292022705\n",
            "Model with relu activation, 100 epoch and 0.1 learning rate :   Train loss:0.33674806356430054   Test loss:0.35159993171691895\n",
            "Model with relu activation, 150 epoch and 0.1 learning rate :   Train loss:0.3286438584327698   Test loss:0.34187689423561096\n",
            "Model with relu activation, 225 epoch and 0.1 learning rate :   Train loss:0.1117573231458664   Test loss:0.14539703726768494\n",
            "Model with relu activation, 100 epoch and 0.2 learning rate :   Train loss:0.2352292835712433   Test loss:0.2231810986995697\n",
            "Model with relu activation, 150 epoch and 0.2 learning rate :   Train loss:0.12153374403715134   Test loss:0.13268549740314484\n",
            "Model with relu activation, 225 epoch and 0.2 learning rate :   Train loss:0.20262324810028076   Test loss:0.2555912137031555\n",
            "Model with tanh activation, 100 epoch and 0.05 learning rate :   Train loss:0.11194945871829987   Test loss:0.12952661514282227\n",
            "Model with tanh activation, 150 epoch and 0.05 learning rate :   Train loss:0.08351477980613708   Test loss:0.09571139514446259\n",
            "Model with tanh activation, 225 epoch and 0.05 learning rate :   Train loss:0.07554465532302856   Test loss:0.0923641100525856\n",
            "Model with tanh activation, 100 epoch and 0.1 learning rate :   Train loss:0.07094822824001312   Test loss:0.0941736027598381\n",
            "Model with tanh activation, 150 epoch and 0.1 learning rate :   Train loss:0.06935729831457138   Test loss:0.10758184641599655\n",
            "Model with tanh activation, 225 epoch and 0.1 learning rate :   Train loss:0.06857974827289581   Test loss:0.08959276229143143\n",
            "Model with tanh activation, 100 epoch and 0.2 learning rate :   Train loss:0.07511471956968307   Test loss:0.08647814393043518\n",
            "Model with tanh activation, 150 epoch and 0.2 learning rate :   Train loss:0.07555132359266281   Test loss:0.07267632335424423\n",
            "Model with tanh activation, 225 epoch and 0.2 learning rate :   Train loss:0.07274805009365082   Test loss:0.10899711400270462\n",
            "Model with sigmoid activation, 100 epoch and 0.05 learning rate :   Train loss:0.6932366490364075   Test loss:0.7327443957328796\n",
            "Model with sigmoid activation, 150 epoch and 0.05 learning rate :   Train loss:0.6927635073661804   Test loss:0.7300496101379395\n",
            "Model with sigmoid activation, 225 epoch and 0.05 learning rate :   Train loss:0.6703101992607117   Test loss:0.7091380953788757\n",
            "Model with sigmoid activation, 100 epoch and 0.1 learning rate :   Train loss:0.6708856225013733   Test loss:0.7083842754364014\n",
            "Model with sigmoid activation, 150 epoch and 0.1 learning rate :   Train loss:0.552801251411438   Test loss:0.5860773324966431\n",
            "Model with sigmoid activation, 225 epoch and 0.1 learning rate :   Train loss:0.348183274269104   Test loss:0.36276501417160034\n",
            "Model with sigmoid activation, 100 epoch and 0.2 learning rate :   Train loss:0.472082257270813   Test loss:0.49199333786964417\n",
            "Model with sigmoid activation, 150 epoch and 0.2 learning rate :   Train loss:0.349443644285202   Test loss:0.36390137672424316\n",
            "Model with sigmoid activation, 225 epoch and 0.2 learning rate :   Train loss:0.3315470814704895   Test loss:0.356063574552536\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "def build_model(act_fn, lr):\n",
        "    optimizer = SGD(learning_rate=lr)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(6, activation=act_fn, input_shape=(8,)))\n",
        "    model.add(Dense(6, activation=act_fn))\n",
        "    model.add(Dense(6, activation=act_fn))\n",
        "    model.add(Dense(6))\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# define activation functions to try\n",
        "act_fns = ['relu', 'tanh', 'sigmoid']\n",
        "\n",
        "# define learning rates to try\n",
        "learning_rates = [0.05, 0.1, 0.2]\n",
        "\n",
        "epochs = [100, 150, 225]\n",
        "\n",
        "models = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for act_fn in act_fns:\n",
        "    for lr in learning_rates:\n",
        "      for epoch in epochs:\n",
        "        print(f'Model with {act_fn} activation, {epoch} epoch and {lr} learning rate')\n",
        "        model = build_model(act_fn, lr)\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epoch)\n",
        "        models.append(f'Model with {act_fn} activation, {epoch} epoch and {lr} learning rate')\n",
        "        train_losses.append(history.history['loss'][-1])\n",
        "        val_losses.append(history.history['val_loss'][-1])\n",
        "\n",
        "for i in range(len(models)):\n",
        "  print(f'{models[i]} :   Train loss:{train_losses[i]}   Test loss:{val_losses[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuqVnA2fgGeN"
      },
      "source": [
        "* Model with relu activation, 100 epoch and 0.05 learning rate :   Train loss:0.48884883522987366   Test loss:0.511837363243103\n",
        "* Model with relu activation, 150 epoch and 0.05 learning rate :   Train loss:0.18912722170352936   Test loss:0.20300935208797455\n",
        "* Model with relu activation, 225 epoch and 0.05 learning rate :   Train loss:0.33096280694007874   Test loss:0.34861767292022705\n",
        "* Model with relu activation, 100 epoch and 0.1 learning rate :   Train loss:0.33674806356430054   Test loss:0.35159993171691895\n",
        "* Model with relu activation, 150 epoch and 0.1 learning rate :   Train loss:0.3286438584327698   Test loss:0.34187689423561096\n",
        "* Model with relu activation, 225 epoch and 0.1 learning rate :   Train loss:0.1117573231458664   Test loss:0.14539703726768494\n",
        "* Model with relu activation, 100 epoch and 0.2 learning rate :   Train loss:0.2352292835712433   Test loss:0.2231810986995697\n",
        "* Model with relu activation, 150 epoch and 0.2 learning rate :   Train loss:0.12153374403715134   Test loss:0.13268549740314484\n",
        "* Model with relu activation, 225 epoch and 0.2 learning rate :   Train loss:0.20262324810028076   Test loss:0.2555912137031555\n",
        "* Model with tanh activation, 100 epoch and 0.05 learning rate :   Train loss:0.11194945871829987   Test loss:0.12952661514282227\n",
        "* Model with tanh activation, 150 epoch and 0.05 learning rate :   Train loss:0.08351477980613708   Test loss:0.09571139514446259\n",
        "* Model with tanh activation, 225 epoch and 0.05 learning rate :   Train loss:0.07554465532302856   Test loss:0.0923641100525856\n",
        "* Model with tanh activation, 100 epoch and 0.1 learning rate :   Train loss:0.07094822824001312   Test loss:0.0941736027598381\n",
        "* Model with tanh activation, 150 epoch and 0.1 learning rate :   Train loss:0.06935729831457138   Test loss:0.10758184641599655\n",
        "* Model with tanh activation, 225 epoch and 0.1 learning rate :   Train loss:0.06857974827289581   Test loss:0.08959276229143143\n",
        "* Model with tanh activation, 100 epoch and 0.2 learning rate :   Train loss:0.07511471956968307   Test loss:0.08647814393043518\n",
        "* Model with tanh activation, 150 epoch and 0.2 learning rate :   Train loss:0.07555132359266281   Test loss:0.07267632335424423\n",
        "* Model with tanh activation, 225 epoch and 0.2 learning rate :   Train loss:0.07274805009365082   Test loss:0.10899711400270462\n",
        "* Model with sigmoid activation, 100 epoch and 0.05 learning rate :   Train loss:0.6932366490364075   Test loss:0.7327443957328796\n",
        "* Model with sigmoid activation, 150 epoch and 0.05 learning rate :   Train loss:0.6927635073661804   Test loss:0.7300496101379395\n",
        "* Model with sigmoid activation, 225 epoch and 0.05 learning rate :   Train loss:0.6703101992607117   Test loss:0.7091380953788757\n",
        "* Model with sigmoid activation, 100 epoch and 0.1 learning rate :   Train loss:0.6708856225013733   Test loss:0.7083842754364014\n",
        "* Model with sigmoid activation, 150 epoch and 0.1 learning rate :   Train loss:0.552801251411438   Test loss:0.5860773324966431\n",
        "* Model with sigmoid activation, 225 epoch and 0.1 learning rate :   Train loss:0.348183274269104   Test loss:0.36276501417160034\n",
        "* Model with sigmoid activation, 100 epoch and 0.2 learning rate :   Train loss:0.472082257270813   Test loss:0.49199333786964417\n",
        "* Model with sigmoid activation, 150 epoch and 0.2 learning rate :   Train loss:0.349443644285202   Test loss:0.36390137672424316\n",
        "* Model with sigmoid activation, 225 epoch and 0.2 learning rate :   Train loss:0.3315470814704895   Test loss:0.356063574552536"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTcCESeQ2V1"
      },
      "source": [
        "# Step 7 - Chosing best parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSdAjoFU41Q"
      },
      "source": [
        "* Model with tanh activation, \n",
        "* 225 epoch\n",
        "* 0.1 learning rate\n",
        "* Train loss:0.06857974827289581        \n",
        "* Test loss:0.08959276229143143"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Z438WRhT84"
      },
      "source": [
        "# Step 8\n",
        "Add new nodes at a time to each hidden layer:\n",
        "* Start from the first hidden layer, add two nodes, train,and record results.\n",
        "* Move to the second hidden layer, add two nodes, train,and record results.\n",
        "* Move to the third hidden layer, add two nodes, train,and record results.\n",
        "* Repeat  Step  8 until bias  and  variance curve  is drawn(see Figure 1for  a fictitiousexample from the first lecture)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKTX8Zg-hoer"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "def build_model(first_node=6, second_node=6, third_node=6):\n",
        "    optimizer = SGD(learning_rate=0.1)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(first_node, activation='tanh', input_shape=(8,)))\n",
        "    model.add(Dense(second_node, activation='tanh'))\n",
        "    model.add(Dense(third_node, activation='tanh'))\n",
        "    model.add(Dense(6))\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LczOHLW6CMsp"
      },
      "source": [
        "* Model with tanh activation, \n",
        "* 225 epoch\n",
        "* 0.1 learning rate\n",
        "* Train loss:0.06857974827289581        \n",
        "* Test loss:0.08959276229143143"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWdje2OxCMXb"
      },
      "source": [
        "* Model with tanh activation, \n",
        "* 225 epoch\n",
        "* 0.1 learning rate\n",
        "* Train loss:0.06857974827289581        \n",
        "* Test loss:0.08959276229143143"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSPicLnRvNmE",
        "outputId": "b64b9cac-2dfe-49ea-80d4-a75f2061deda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with 6 nodes in first hidden layer, 6 nodes in second hidden layer, 6 nodes in third hidden layer\n",
            "Training model with 8 nodes in first hidden layer, 8 nodes in second hidden layer, 8 nodes in third hidden layer\n",
            "Training model with 10 nodes in first hidden layer, 10 nodes in second hidden layer, 10 nodes in third hidden layer\n",
            "Training model with 12 nodes in first hidden layer, 12 nodes in second hidden layer, 12 nodes in third hidden layer\n",
            "Training model with 14 nodes in first hidden layer, 14 nodes in second hidden layer, 14 nodes in third hidden layer\n",
            "Training model with 16 nodes in first hidden layer, 16 nodes in second hidden layer, 16 nodes in third hidden layer\n",
            "Training model with 18 nodes in first hidden layer, 18 nodes in second hidden layer, 18 nodes in third hidden layer\n",
            "Training model with 20 nodes in first hidden layer, 20 nodes in second hidden layer, 20 nodes in third hidden layer\n",
            "Training model with 22 nodes in first hidden layer, 22 nodes in second hidden layer, 22 nodes in third hidden layer\n",
            "Training model with 24 nodes in first hidden layer, 24 nodes in second hidden layer, 24 nodes in third hidden layer\n",
            "Training model with 26 nodes in first hidden layer, 26 nodes in second hidden layer, 26 nodes in third hidden layer\n",
            "Training model with 28 nodes in first hidden layer, 28 nodes in second hidden layer, 28 nodes in third hidden layer\n",
            "Training model with 30 nodes in first hidden layer, 30 nodes in second hidden layer, 30 nodes in third hidden layer\n",
            "Training model with 32 nodes in first hidden layer, 32 nodes in second hidden layer, 32 nodes in third hidden layer\n",
            "Training model with 34 nodes in first hidden layer, 34 nodes in second hidden layer, 34 nodes in third hidden layer\n",
            "Training model with 36 nodes in first hidden layer, 36 nodes in second hidden layer, 36 nodes in third hidden layer\n",
            "Training model with 38 nodes in first hidden layer, 38 nodes in second hidden layer, 38 nodes in third hidden layer\n",
            "Training model with 40 nodes in first hidden layer, 40 nodes in second hidden layer, 40 nodes in third hidden layer\n",
            "Training model with 42 nodes in first hidden layer, 42 nodes in second hidden layer, 42 nodes in third hidden layer\n",
            "Training model with 44 nodes in first hidden layer, 44 nodes in second hidden layer, 44 nodes in third hidden layer\n",
            "Training model with 46 nodes in first hidden layer, 46 nodes in second hidden layer, 46 nodes in third hidden layer\n",
            "Training model with 48 nodes in first hidden layer, 48 nodes in second hidden layer, 48 nodes in third hidden layer\n",
            "Training model with 50 nodes in first hidden layer, 50 nodes in second hidden layer, 50 nodes in third hidden layer\n",
            "Training model with 52 nodes in first hidden layer, 52 nodes in second hidden layer, 52 nodes in third hidden layer\n",
            "Training model with 54 nodes in first hidden layer, 54 nodes in second hidden layer, 54 nodes in third hidden layer\n",
            "Training model with 56 nodes in first hidden layer, 56 nodes in second hidden layer, 56 nodes in third hidden layer\n",
            "Training model with 58 nodes in first hidden layer, 58 nodes in second hidden layer, 58 nodes in third hidden layer\n",
            "Training model with 60 nodes in first hidden layer, 60 nodes in second hidden layer, 60 nodes in third hidden layer\n",
            "Training model with 62 nodes in first hidden layer, 62 nodes in second hidden layer, 62 nodes in third hidden layer\n",
            "Training model with 64 nodes in first hidden layer, 64 nodes in second hidden layer, 64 nodes in third hidden layer\n",
            "Training model with 66 nodes in first hidden layer, 66 nodes in second hidden layer, 66 nodes in third hidden layer\n",
            "Training model with 68 nodes in first hidden layer, 68 nodes in second hidden layer, 68 nodes in third hidden layer\n",
            "Training model with 70 nodes in first hidden layer, 70 nodes in second hidden layer, 70 nodes in third hidden layer\n",
            "Training model with 72 nodes in first hidden layer, 72 nodes in second hidden layer, 72 nodes in third hidden layer\n",
            "Training model with 74 nodes in first hidden layer, 74 nodes in second hidden layer, 74 nodes in third hidden layer\n",
            "Training model with 76 nodes in first hidden layer, 76 nodes in second hidden layer, 76 nodes in third hidden layer\n",
            "Training model with 78 nodes in first hidden layer, 78 nodes in second hidden layer, 78 nodes in third hidden layer\n",
            "Training model with 80 nodes in first hidden layer, 80 nodes in second hidden layer, 80 nodes in third hidden layer\n",
            "Training model with 82 nodes in first hidden layer, 82 nodes in second hidden layer, 82 nodes in third hidden layer\n",
            "Training model with 84 nodes in first hidden layer, 84 nodes in second hidden layer, 84 nodes in third hidden layer\n",
            "Training model with 86 nodes in first hidden layer, 86 nodes in second hidden layer, 86 nodes in third hidden layer\n",
            "Training model with 88 nodes in first hidden layer, 88 nodes in second hidden layer, 88 nodes in third hidden layer\n",
            "Training model with 90 nodes in first hidden layer, 90 nodes in second hidden layer, 90 nodes in third hidden layer\n",
            "Training model with 92 nodes in first hidden layer, 92 nodes in second hidden layer, 92 nodes in third hidden layer\n",
            "Training model with 94 nodes in first hidden layer, 94 nodes in second hidden layer, 94 nodes in third hidden layer\n",
            "Training model with 96 nodes in first hidden layer, 96 nodes in second hidden layer, 96 nodes in third hidden layer\n",
            "Training model with 98 nodes in first hidden layer, 98 nodes in second hidden layer, 98 nodes in third hidden layer\n",
            "Training model with 100 nodes in first hidden layer, 100 nodes in second hidden layer, 100 nodes in third hidden layer\n",
            "Training model with 102 nodes in first hidden layer, 102 nodes in second hidden layer, 102 nodes in third hidden layer\n",
            "Training model with 104 nodes in first hidden layer, 104 nodes in second hidden layer, 104 nodes in third hidden layer\n",
            "Training model with 106 nodes in first hidden layer, 106 nodes in second hidden layer, 106 nodes in third hidden layer\n",
            "Training model with 108 nodes in first hidden layer, 108 nodes in second hidden layer, 108 nodes in third hidden layer\n",
            "Training model with 110 nodes in first hidden layer, 110 nodes in second hidden layer, 110 nodes in third hidden layer\n",
            "Training model with 112 nodes in first hidden layer, 112 nodes in second hidden layer, 112 nodes in third hidden layer\n",
            "Training model with 114 nodes in first hidden layer, 114 nodes in second hidden layer, 114 nodes in third hidden layer\n",
            "Training model with 116 nodes in first hidden layer, 116 nodes in second hidden layer, 116 nodes in third hidden layer\n",
            "Training model with 118 nodes in first hidden layer, 118 nodes in second hidden layer, 118 nodes in third hidden layer\n",
            "Training model with 120 nodes in first hidden layer, 120 nodes in second hidden layer, 120 nodes in third hidden layer\n",
            "Training model with 122 nodes in first hidden layer, 122 nodes in second hidden layer, 122 nodes in third hidden layer\n",
            "Training model with 124 nodes in first hidden layer, 124 nodes in second hidden layer, 124 nodes in third hidden layer\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "first_node = 6\n",
        "second_node = 6\n",
        "third_node = 6\n",
        "\n",
        "for i in range(0, 120, 2):\n",
        "    print(f'Training model with {first_node+i} nodes in first hidden layer, {second_node+i} nodes in second hidden layer, {third_node+i} nodes in third hidden layer')\n",
        "    model = build_model(first_node+i, second_node+i, third_node+i)\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=225, verbose=0)\n",
        "    train_losses.append(history.history['loss'][-1])\n",
        "    val_losses.append(history.history['val_loss'][-1])\n",
        "\n",
        "# Plot bias and variance curve\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(val_losses, label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}